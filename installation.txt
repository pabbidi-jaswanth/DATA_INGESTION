# app/utils/embeddings.py
# Unified embedding provider with offline fallback.
# Tries SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2").
# If that fails (SSL/proxy/no-internet), falls back to TF-IDF (scikit-learn).
#
# For TF-IDF, we must persist the vectorizer per collection directory
# so that query-time uses the exact same vocabulary.

from __future__ import annotations

import pathlib
import pickle
from typing import List, Optional, Tuple

import numpy as np

_EMB_DIM = 384  # for MiniLM if available

class _STEncoder:
    def __init__(self):
        from sentence_transformers import SentenceTransformer
        self.model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

    @property
    def is_transformer(self) -> bool:
        return True

    def fit(self, texts: List[str]) -> None:
        # No fit needed for ST
        pass

    def encode(self, texts: List[str]) -> np.ndarray:
        # normalize embeddings=True returns unit vectors
        embs = self.model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)
        return embs.astype(np.float32)

    def save(self, dest_dir: pathlib.Path) -> None:
        # Nothing to save for ST
        pass

    @staticmethod
    def load(dest_dir: pathlib.Path) -> "_STEncoder":
        # ST is loaded from hub (if available); if not available, load will fail.
        return _STEncoder()


class _TFIDFEncoder:
    def __init__(self, vectorizer=None):
        self.vectorizer = vectorizer  # can be None initially

    @property
    def is_transformer(self) -> bool:
        return False

    def fit(self, texts: List[str]) -> None:
        # Fit a TF-IDF vectorizer on texts
        from sklearn.feature_extraction.text import TfidfVectorizer
        # reasonable defaults; adjust if needed
        self.vectorizer = TfidfVectorizer(
            lowercase=True,
            stop_words="english",
            max_features=50000,
            ngram_range=(1, 2),
        )
        self.vectorizer.fit(texts)

    def encode(self, texts: List[str]) -> np.ndarray:
        if self.vectorizer is None:
            raise RuntimeError("TF-IDF encoder not fitted.")
        X = self.vectorizer.transform(texts)
        # Convert sparse to dense (FAISS needs dense). L2 normalize rows.
        X = X.astype(np.float32).toarray()
        norms = np.linalg.norm(X, axis=1, keepdims=True) + 1e-12
        X = X / norms
        return X

    def save(self, dest_dir: pathlib.Path) -> None:
        dest_dir.mkdir(parents=True, exist_ok=True)
        with open(dest_dir / "vectorizer.pkl", "wb") as f:
            pickle.dump(self.vectorizer, f)

    @staticmethod
    def load(dest_dir: pathlib.Path) -> "_TFIDFEncoder":
        p = dest_dir / "vectorizer.pkl"
        if not p.exists():
            raise FileNotFoundError(f"TF-IDF vectorizer not found at {p}")
        with open(p, "rb") as f:
            vec = pickle.load(f)
        return _TFIDFEncoder(vectorizer=vec)


def get_encoder_for_build(prefer_transformer: bool = True):
    """
    Called at build-time. Try ST first; if it fails (network/SSL), use TF-IDF.
    Returns an encoder instance and a flag 'used_fallback'.
    """
    if prefer_transformer:
        try:
            enc = _STEncoder()
            # quick smoke encode (tiny) to ensure model is ready
            _ = enc.encode(["ok"])
            return enc, False
        except Exception:
            pass
    # fallback
    return _TFIDFEncoder(), True


def get_encoder_for_query(collection_dir: pathlib.Path):
    """
    Called at query-time. If a TF-IDF vectorizer exists in the collection folder,
    use TF-IDF; otherwise try ST.
    """
    vec_path = collection_dir / "vectorizer.pkl"
    if vec_path.exists():
        return _TFIDFEncoder.load(collection_dir)
    # else try ST
    try:
        return _STEncoder()
    except Exception as e:
        raise RuntimeError(
            f"No TF-IDF vectorizer found at {vec_path} and could not load ST model: {e}"
        )




# ETL/build_faiss_collections.py
# Build 5 FAISS collections with metadata (UG/PG/MCA/MSc/HOSTELS).
# Offline-safe: falls back to TF-IDF if SentenceTransformer can't be downloaded.
# Input PDFs expected under Data/Raw/ACADEMICS and Data/Raw/HOSTEL

import re, json, pathlib
from typing import List, Dict
import pdfplumber
import numpy as np
import faiss

from app.utils.embeddings import get_encoder_for_build

ROOT = pathlib.Path(__file__).resolve().parents[1]
RAW_ACAD = ROOT / "Data" / "Raw" / "ACADEMICS"
RAW_HOST = ROOT / "Data" / "Raw" / "HOSTEL"
OUT_DIR  = ROOT / "Data" / "index" / "faiss"

# Map PDFs → section
SECTION_MAP = {
    "UG":      ["UG", "UG ", "UNDERGRAD", "VITEEE", "NRI", "FOREIGN"],
    "PG":      ["PG", "M.TECH", "MBA", "VITMEE", "VITREE"],
    "MCA":     ["MCA"],
    "MSc":     ["M.SC", "MSC", "SCIENCE"],
    "HOSTELS": ["HOSTEL", "HOSTELS", "LADIES", "MENS", "MEN'S"],
}

CHUNK_SIZE = 900
CHUNK_OVERLAP = 150

def read_pdf_text(pdf_path: pathlib.Path) -> str:
    parts = []
    with pdfplumber.open(str(pdf_path)) as pdf:
        for p in pdf.pages:
            t = p.extract_text() or ""
            parts.append(t)
    return "\n".join(parts)

def clean_text(t: str) -> str:
    # simple OCR cleanup: collapse hyphen linebreaks, fix multi-spaces, normalize dash
    t = re.sub(r"(\w)-\n(\w)", r"\1\2", t)
    t = re.sub(r"\n+", "\n", t)
    t = t.replace("–", "-").replace("—", "-")
    t = re.sub(r"[ \t]+", " ", t)
    return t.strip()

def chunk_text(t: str, size=CHUNK_SIZE, overlap=CHUNK_OVERLAP) -> List[str]:
    tokens = t.split()
    chunks = []
    i = 0
    while i < len(tokens):
        j = min(len(tokens), i + size)
        chunk = " ".join(tokens[i:j])
        chunks.append(chunk)
        i = j - overlap
        if i < 0: i = 0
        if j == len(tokens): break
    return [c for c in chunks if c.strip()]

def decide_section(doc_name: str) -> str:
    upper = doc_name.upper()
    for sec, keys in SECTION_MAP.items():
        if any(k in upper for k in keys):
            return sec
    return "UG"

def collect_docs() -> List[Dict]:
    docs = []
    for base in [RAW_ACAD, RAW_HOST]:
        if not base.exists(): 
            continue
        for p in base.glob("**/*.pdf"):
            section = decide_section(p.name)
            docs.append({"path": p, "section": section})
    return docs

def build_one_collection(section: str, docs: List[Dict], encoder, used_fallback: bool):
    texts, metas = [], []
    for d in docs:
        text = clean_text(read_pdf_text(d["path"]))
        for ch in chunk_text(text):
            texts.append(ch)
            metas.append({"section": section, "doc_name": d["path"].name})

    out_dir = OUT_DIR / f"{'vit_'+section.lower()}"
    out_dir.mkdir(parents=True, exist_ok=True)

    if not texts:
        print(f"[{section}] No texts; skipping.")
        return

    # If TF-IDF fallback is used, we must fit the vectorizer on the corpus
    if not getattr(encoder, "is_transformer", False):
        encoder.fit(texts)
        encoder.save(out_dir)  # persist vectorizer.pkl for this collection

    embs = encoder.encode(texts)  # (n, d)
    # Use Inner Product (cosine if normalized). Ensure float32
    index = faiss.IndexFlatIP(embs.shape[1])
    index.add(embs.astype(np.float32))

    faiss.write_index(index, str(out_dir / "index.faiss"))
    (out_dir / "texts.json").write_text(json.dumps(texts, ensure_ascii=False), encoding="utf-8")
    (out_dir / "metas.json").write_text(json.dumps(metas, ensure_ascii=False), encoding="utf-8")

    print(f"[{section}] built: {len(texts)} chunks → {out_dir} | encoder={'TF-IDF' if used_fallback else 'ST'}")

def main():
    OUT_DIR.mkdir(parents=True, exist_ok=True)
    docs = collect_docs()
    by_sec = {"UG":[], "PG":[], "MCA":[], "MSc":[], "HOSTELS":[]}
    for d in docs:
        by_sec[d["section"]].append(d)

    encoder, used_fallback = get_encoder_for_build(prefer_transformer=True)

    for sec in by_sec:
        build_one_collection(sec, by_sec[sec], encoder, used_fallback)

if __name__ == "__main__":
    main()





# app/utils/fallback_rag.py
# Section-aware FAISS retrieval that works with either:
# - SentenceTransformer embeddings, or
# - TF-IDF fallback (if vectorizer.pkl exists in the collection folder).

import json, pathlib
from typing import List
import numpy as np
import faiss

from app.utils.embeddings import get_encoder_for_query

def _paths(base_dir: pathlib.Path, collection: str):
    d = base_dir / collection
    return d, d / "index.faiss", d / "texts.json", d / "metas.json"

def _mmr_select(embs, query_vec, k=6, lambda_div=0.65):
    # cosine already; embs and query_vec assumed normalized
    sims = np.dot(embs, query_vec)
    selected = []
    candidates = list(range(len(sims)))
    while candidates and len(selected) < k:
        if not selected:
            idx = int(np.argmax(sims[candidates]))
            selected.append(candidates[idx])
            candidates.pop(idx)
        else:
            q_scores = sims[candidates]
            div_scores = []
            for c in candidates:
                div_scores.append(max([float(np.dot(embs[c], embs[s])) for s in selected]))
            div_scores = np.array(div_scores)
            mmr = lambda_div * q_scores - (1 - lambda_div) * div_scores
            idx = int(np.argmax(mmr))
            selected.append(candidates[idx])
            candidates.pop(idx)
    return selected

def faiss_answer_or_summary(base_dir, collection, query: str, top_k: int = 10) -> str:
    base_dir = pathlib.Path(base_dir)
    col_dir, fa, ft, fm = _paths(base_dir, collection)
    index = faiss.read_index(str(fa))
    texts = json.loads(pathlib.Path(ft).read_text(encoding="utf-8"))
    metas = json.loads(pathlib.Path(fm).read_text(encoding="utf-8"))

    encoder = get_encoder_for_query(col_dir)  # will load TF-IDF if present, else ST

    # Encode query
    q = encoder.encode([query])[0]  # (d,)
    q = q.astype(np.float32)

    D, I = index.search(q.reshape(1, -1), top_k)
    I = I[0].tolist()
    if not I:
        return ""

    # re-rank with a small MMR on a pool
    pool = I + [i for i in range(len(texts)) if i not in I][:top_k]
    pool_texts = [texts[i] for i in pool]
    pool_embs = encoder.encode(pool_texts)
    # ensure L2-normalized (should already be)
    norms = np.linalg.norm(pool_embs, axis=1, keepdims=True) + 1e-12
    pool_embs = pool_embs / norms

    q_norm = q / (np.linalg.norm(q) + 1e-12)
    sel_idx = _mmr_select(pool_embs, q_norm, k=min(6, len(pool)))
    chosen = [pool[i] for i in sel_idx]

    out_lines = []
    for i in chosen:
        t = texts[i]
        m = metas[i]
        doc = m.get("doc_name", "source")
        out_lines.append(f"- **{doc}**: {t[:1000]}{'...' if len(t)>1000 else ''}")
    return "\n".join(out_lines)



