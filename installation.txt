# ETL/build_faiss_collections.py
# Build 5 FAISS collections with metadata (UG/PG/MCA/MSc/HOSTELS).
# Offline-safe: TF-IDF ONLY. Scans ALL of Data/Raw/**.pdf and classifies by path/name.
# Adds robust text extraction with optional OCR (pytesseract).

import sys, os, re, json, pathlib, io
from typing import List, Dict, Tuple

ROOT = pathlib.Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

import numpy as np
import faiss
import pdfplumber
from pypdf import PdfReader

# optional OCR
try:
    import pytesseract
    from PIL import Image
    TESS_OK = True
except Exception:
    TESS_OK = False

# our local offline encoder (TF-IDF) helpers
from app.utils.embeddings import get_encoder_for_build

RAW_ALL = ROOT / "Data" / "Raw"
OUT_DIR  = ROOT / "Data" / "index" / "faiss"

# Prefer folder hints first (section if any path part contains these tokens)
FOLDER_HINTS = {
    "UG":      ["UG"],
    "PG":      ["PG"],
    "MCA":     ["MCA"],
    "MSc":     ["MSC", "M.SC", "MSCI"],
    "HOSTELS": ["HOSTEL", "HOSTELS", "LADIES", "MENS", "MEN'S"],
}
# Fallback to filename hints
FILENAME_HINTS = {
    "UG":      ["UG", "UNDERGRAD", "VITEEE", "NRI", "FOREIGN"],
    "PG":      ["PG", "M.TECH", "MBA", "VITMEE", "VITREE"],
    "MCA":     ["MCA"],
    "MSc":     ["M.SC", "MSC", "SCIENCE"],
    "HOSTELS": ["HOSTEL", "HOSTELS", "LADIES", "MENS", "MEN'S"],
}

CHUNK_SIZE = 900
CHUNK_OVERLAP = 150
MIN_TEXT_CHARS = 400   # if below this after pdfplumber+pypdf → we try OCR

def _contains_any(s: str, keys: List[str]) -> bool:
    return any(k in s for k in keys)

def section_from_path(p: pathlib.Path) -> str | None:
    rel = p.relative_to(ROOT).as_posix().upper()
    for sec, hints in FOLDER_HINTS.items():
        if _contains_any(rel, [f"/{h}/" for h in hints]) or _contains_any(rel, [f"/{h}." for h in hints]):
            return sec
    return None

def section_from_filename(name: str) -> str | None:
    upper = name.upper()
    for sec, keys in FILENAME_HINTS.items():
        if any(k in upper for k in keys):
            return sec
    return None

def decide_section(pdf_path: pathlib.Path) -> str:
    s = section_from_path(pdf_path)
    if s: return s
    s = section_from_filename(pdf_path.name)
    if s: return s
    return "UG"

def clean_text(t: str) -> str:
    # collapse hyphen linebreaks, normalize whitespace and dashes
    t = re.sub(r"(\w)-\n(\w)", r"\1\2", t)
    t = t.replace("–", "-").replace("—", "-")
    t = re.sub(r"[ \t]+", " ", t)
    t = re.sub(r"\n{3,}", "\n\n", t)
    return t.strip()

def chunk_text(t: str, size=CHUNK_SIZE, overlap=CHUNK_OVERLAP) -> List[str]:
    tokens = t.split()
    chunks, i = [], 0
    while i < len(tokens):
        j = min(len(tokens), i + size)
        chunks.append(" ".join(tokens[i:j]))
        i = j - overlap
        if i < 0: i = 0
        if j == len(tokens): break
    return [c for c in chunks if c.strip()]

def extract_with_pdfplumber(pdf_path: pathlib.Path) -> str:
    parts = []
    try:
        with pdfplumber.open(str(pdf_path)) as pdf:
            for p in pdf.pages:
                t = p.extract_text() or ""
                parts.append(t)
    except Exception:
        return ""
    return "\n".join(parts)

def extract_with_pypdf(pdf_path: pathlib.Path) -> str:
    parts = []
    try:
        reader = PdfReader(str(pdf_path))
        for page in reader.pages:
            t = page.extract_text() or ""
            parts.append(t)
    except Exception:
        return ""
    return "\n".join(parts)

def extract_with_ocr(pdf_path: pathlib.Path) -> str:
    if not TESS_OK:
        return ""
    try:
        text_parts = []
        with pdfplumber.open(str(pdf_path)) as pdf:
            for p in pdf.pages:
                # render to image (150 dpi keeps it lighter)
                im = p.to_image(resolution=150).original  # PIL Image
                t = pytesseract.image_to_string(im) or ""
                text_parts.append(t)
        return "\n".join(text_parts)
    except Exception:
        return ""

def read_pdf_text(pdf_path: pathlib.Path) -> Tuple[str, Dict[str, int]]:
    """
    Return best-effort text and a dict with char counts per method for logging.
    """
    log = {"plumber": 0, "pypdf": 0, "ocr": 0}
    t1 = extract_with_pdfplumber(pdf_path)
    log["plumber"] = len(t1)
    if len(t1) >= MIN_TEXT_CHARS:
        return t1, log

    t2 = extract_with_pypdf(pdf_path)
    log["pypdf"] = len(t2)
    if len(t2) >= MIN_TEXT_CHARS:
        return t2, log

    t3 = extract_with_ocr(pdf_path)
    log["ocr"] = len(t3)
    # Even if OCR is short, we return it (might be small, but better than nothing)
    best = max([(len(t1), t1), (len(t2), t2), (len(t3), t3)], key=lambda x: x[0])[1]
    return best, log

def collect_pdfs() -> List[pathlib.Path]:
    if not RAW_ALL.exists():
        return []
    return sorted(RAW_ALL.glob("**/*.pdf"))

def gather_docs() -> Tuple[Dict[str, List[Dict]], List[Tuple[str, str]]]:
    by_sec = {"UG": [], "PG": [], "MCA": [], "MSc": [], "HOSTELS": []}
    mapping_log = []
    for p in collect_pdfs():
        sec = decide_section(p)
        by_sec[sec].append({"path": p, "section": sec})
        mapping_log.append((sec, p.relative_to(ROOT).as_posix()))
    return by_sec, mapping_log

def build_one_collection(section: str, docs: List[Dict], encoder):
    texts, metas = [], []

    print(f"\n[{section}] extracting {len(docs)} PDF(s)…")
    for d in docs:
        raw_text, counts = read_pdf_text(d["path"])
        print(f"  - {d['path'].relative_to(ROOT).as_posix()}  "
              f"(chars: plumber={counts['plumber']}, pypdf={counts['pypdf']}, ocr={counts['ocr']})")
        text = clean_text(raw_text)
        if not text:
            continue
        for ch in chunk_text(text):
            texts.append(ch)
            metas.append({"section": section, "doc_name": d["path"].name})

    out_dir = OUT_DIR / f"{'vit_'+section.lower()}"
    out_dir.mkdir(parents=True, exist_ok=True)

    if not texts:
        print(f"[{section}] No extracted text; skipping index.")
        return

    # fit + save TF-IDF encoder ON THIS SECTION
    section_encoder = get_encoder_for_build()
    section_encoder.fit(texts)
    section_encoder.save(out_dir)

    embs = section_encoder.encode(texts)  # L2-normalized
    index = faiss.IndexFlatIP(embs.shape[1])  # cosine if normalized
    index.add(embs.astype(np.float32))

    faiss.write_index(index, str(out_dir / "index.faiss"))
    (out_dir / "texts.json").write_text(json.dumps(texts, ensure_ascii=False), encoding="utf-8")
    (out_dir / "metas.json").write_text(json.dumps(metas, ensure_ascii=False), encoding="utf-8")

    print(f"[{section}] built: {len(texts)} chunks → {out_dir} | encoder=TF-IDF")

def main():
    OUT_DIR.mkdir(parents=True, exist_ok=True)
    by_sec, mapping_log = gather_docs()

    print("\n=== PDF → Section mapping ===")
    for sec, rel in mapping_log:
        print(f"{sec:8s} | {rel}")
    print("=============================\n")

    for sec in ["UG", "PG", "MCA", "MSc", "HOSTELS"]:
        build_one_collection(sec, by_sec[sec], encoder=None)

if __name__ == "__main__":
    main()
