#!/usr/bin/env python3
# embeddings_faiss_gemini.py
"""
Build FAISS index from chunks_clean_final.jsonl using Gemini embeddings.

Usage:
    python embeddings_faiss_gemini.py \
        --chunks Data/processed/chunks_clean_final.jsonl \
        --persist Data/index/faiss \
        --collection vit_faq_vellore
"""

import os, sys, json, argparse
from pathlib import Path
from tqdm import tqdm
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Import LangChain and Gemini
from langchain_community.vectorstores import FAISS
from langchain_core.embeddings import Embeddings
import google.generativeai as genai

# ==================== Gemini Embeddings ====================

class GeminiEmbeddings(Embeddings):
    """
    LangChain-compatible Gemini embeddings using text-embedding-004.
    """
    def __init__(self, model: str = "models/text-embedding-004"):
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            print("[ERROR] GEMINI_API_KEY not set in environment or .env file")
            sys.exit(1)
        
        genai.configure(api_key=api_key)
        self.genai = genai
        self.model = model
        print(f"[OK] Gemini embeddings initialized with model: {model}")
    
    def embed_documents(self, texts: list[str]) -> list[list[float]]:
        """Embed multiple documents"""
        embeddings = []
        print(f"[INFO] Embedding {len(texts)} documents...")
        
        for i, text in enumerate(tqdm(texts, desc="Embedding")):
            try:
                result = self.genai.embed_content(model=self.model, content=text)
                embedding = result.get("embedding")
                
                if embedding is None:
                    print(f"[WARN] No embedding for doc {i}, using zeros")
                    embedding = [0.0] * 768  # Default dimension
                
                embeddings.append(embedding)
                
            except Exception as e:
                print(f"[ERROR] Failed to embed doc {i}: {e}")
                embeddings.append([0.0] * 768)
        
        return embeddings
    
    def embed_query(self, text: str) -> list[float]:
        """Embed a single query"""
        try:
            result = self.genai.embed_content(model=self.model, content=text)
            embedding = result.get("embedding")
            
            if embedding is None:
                raise RuntimeError("Gemini returned no embedding for query")
            
            return embedding
            
        except Exception as e:
            print(f"[ERROR] Failed to embed query: {e}")
            raise

# ==================== Data Loading ====================

def load_chunks(jsonl_path: Path) -> tuple[list[str], list[dict], list[str]]:
    """
    Load chunks from JSONL file.
    Returns: (texts, metadatas, ids)
    """
    texts, metas, ids = [], [], []
    
    print(f"[INFO] Loading chunks from: {jsonl_path}")
    
    with jsonl_path.open("r", encoding="utf-8") as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue
            
            try:
                obj = json.loads(line)
                text = (obj.get("text") or "").strip()
                
                if not text:
                    continue
                
                texts.append(text)
                metas.append(obj.get("metadata", {}))
                ids.append(obj.get("id") or f"chunk_{len(ids)}")
                
            except json.JSONDecodeError as e:
                print(f"[WARN] Invalid JSON at line {line_num}: {e}")
                continue
    
    print(f"[OK] Loaded {len(texts)} chunks")
    return texts, metas, ids

# ==================== FAISS Building ====================

def build_faiss(texts: list[str], metas: list[dict], ids: list[str], 
                persist_dir: Path, collection: str, embedder: GeminiEmbeddings):
    """
    Build and save FAISS index.
    """
    if not texts:
        print("[ERROR] No texts to index!")
        sys.exit(1)
    
    print(f"[INFO] Building FAISS index with {len(texts)} documents...")
    
    # Create FAISS store (automatically embeds during creation)
    store = FAISS.from_texts(
        texts=texts,
        embedding=embedder,
        metadatas=metas
    )
    
    # Save to disk
    persist_dir.mkdir(parents=True, exist_ok=True)
    store.save_local(str(persist_dir), index_name=collection)
    
    print(f"[DONE] FAISS index saved to: {persist_dir}/{collection}.faiss")
    print(f"[DONE] Metadata saved to: {persist_dir}/{collection}.pkl")
    print(f"[INFO] Total documents indexed: {len(texts)}")

# ==================== Main ====================

def main():
    parser = argparse.ArgumentParser(
        description="Build FAISS index from chunks using Gemini embeddings"
    )
    parser.add_argument(
        "--chunks",
        required=True,
        help="Path to chunks_clean_final.jsonl"
    )
    parser.add_argument(
        "--persist",
        required=True,
        help="Directory to save FAISS index (e.g., Data/index/faiss)"
    )
    parser.add_argument(
        "--collection",
        default="vit_faq_vellore",
        help="Index name (default: vit_faq_vellore)"
    )
    
    args = parser.parse_args()
    
    # Validate paths
    chunks_path = Path(args.chunks).resolve()
    if not chunks_path.exists():
        print(f"[ERROR] Chunks file not found: {chunks_path}")
        sys.exit(1)
    
    persist_dir = Path(args.persist).resolve()
    
    # Load data
    texts, metas, ids = load_chunks(chunks_path)
    
    if not texts:
        print("[ERROR] No valid chunks found in JSONL file")
        sys.exit(1)
    
    # Initialize embeddings
    embedder = GeminiEmbeddings()
    
    # Build and save FAISS
    build_faiss(texts, metas, ids, persist_dir, args.collection, embedder)
    
    # Verification
    print("\n" + "="*60)
    print("[VERIFY] Testing the saved index...")
    try:
        from langchain_community.vectorstores import FAISS
        
        loaded_store = FAISS.load_local(
            str(persist_dir),
            embeddings=embedder,
            index_name=args.collection,
            allow_dangerous_deserialization=True
        )
        
        test_query = "What is VITMEE?"
        results = loaded_store.similarity_search(test_query, k=2)
        
        print(f"[OK] Index loaded successfully")
        print(f"[OK] Test query: '{test_query}'")
        print(f"[OK] Retrieved {len(results)} results")
        
        if results:
            print(f"\nSample result:")
            print(f"  Text: {results[0].page_content[:150]}...")
            print(f"  Source: {results[0].metadata.get('source_file', 'unknown')}")
        
    except Exception as e:
        print(f"[ERROR] Verification failed: {e}")
    
    print("="*60)

if __name__ == "__main__":
    main()
