#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Module-1: Data Ingestion for VIT Smart FAQ (Vellore MVP)

- Input  : data/raw/  (drop all PDFs/DOCX here)
- Output : data/processed/chunks.jsonl  (one JSON per chunk)
           data/processed/catalog.csv   (one row per source file)

Requires: pdfplumber, python-docx, langchain, tqdm
    pip install pdfplumber python-docx langchain tqdm
"""

import os, re, json, csv, argparse, hashlib, datetime, sys
from pathlib import Path
from typing import Dict, Any, List, Optional

# PDF & DOCX extractors
import pdfplumber
try:
    import docx  # python-docx
    HAS_DOCX = True
except Exception:
    HAS_DOCX = False

# Chunking (heading-aware)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from tqdm import tqdm


# ------------------------------- Cleaning ---------------------------------- #
PAGE_PATTERNS = [
    r"Page\s+\d+\s*(?:of|/)\s*\d+",
    r"^\s*Vellore\s+Institute\s+of\s+Technology.*$",     # common letterheads (broad)
    r"^\s*L.?&.?T\s*EduTech.*$",
]
PAGE_REGEXES = [re.compile(p, re.IGNORECASE | re.MULTILINE) for p in PAGE_PATTERNS]

def clean_text(raw: str) -> str:
    """Light cleanup: remove common headers/footers, fix hyphenation,
    collapse single newlines (keep double as paragraph), normalize spaces."""
    if not raw:
        return ""

    # Normalize newlines
    txt = raw.replace("\r\n", "\n").replace("\r", "\n")

    # Drop frequent page artifacts
    for rgx in PAGE_REGEXES:
        txt = rgx.sub("", txt)

    # Remove lines that are just page numbers or whitespace
    txt = re.sub(r"^\s*\d+\s*$", "", txt, flags=re.MULTILINE)

    # Fix hyphenated line breaks: "admis-\nsion" -> "admission"
    txt = re.sub(r"(\w)-\n(\w)", r"\1\2", txt)

    # Keep paragraph breaks but join within paragraphs:
    # replace single \n with space; keep double \n\n
    txt = re.sub(r"(?<!\n)\n(?!\n)", " ", txt)

    # Collapse 3+ newlines to 2
    txt = re.sub(r"\n{3,}", "\n\n", txt)

    # Collapse repeated spaces/tabs
    txt = re.sub(r"[ \t]{2,}", " ", txt)

    # Trim
    txt = txt.strip()
    return txt


# ------------------------------ Metadata ----------------------------------- #
def _guess_ay_from_name(name: str) -> Optional[str]:
    # Try patterns like 2025-26, 2025–26, 25-26, 2025_26
    name = name.replace("_", "-")
    m = re.search(r"(20\d{2})\D{0,3}(\d{2})", name)
    if m:
        return f"{m.group(1)}-{m.group(2)}"
    return None

def _bool(val):
    return True if val else False

def map_metadata(filename: str) -> Dict[str, Any]:
    """Infer metadata from filename (lowercased)."""
    n = filename.lower()

    meta: Dict[str, Any] = {
        "campus": "Vellore",       # MVP fixed
        "domain": None,            # UG | PG | Research | Hostel | NRI-UG | Foreign-UG | NRI-Intl
        "category": None,          # Fee Structure | Eligibility | Documents Required | Courses Offered | Academic Rules | Hostel Norms | Refund Policy | Contacts | FAQ | Admission Process | Exam Pattern
        "program": None,           # B.Tech | M.Tech | MCA | M.Sc | Ph.D. | Direct Ph.D.
        "gender": None,            # Male | Female (hostel)
        "student_year": None,      # First-Year | Senior (hostel)
        "currency": None,          # INR | USD
        "audience": None,          # NRI | Foreign (if needed)
        "ay": _guess_ay_from_name(n),
        "stable_flag": "stable",   # fees/refunds/exam windows -> volatile
    }

    # --- Domain ---
    if "vitree" in n or "phd" in n or "ph.d" in n or "research" in n:
        meta["domain"] = "Research"
        if "direct" in n:
            meta["program"] = "Direct Ph.D."
        else:
            meta["program"] = "Ph.D."
    elif n.startswith("mh") or n.startswith("lh") or "hostel" in n:
        meta["domain"] = "Hostel"
    elif "nri" in n and "ug" in n:
        meta["domain"] = "NRI-UG"
        meta["audience"] = "NRI"
    elif "foreign" in n and "ug" in n:
        meta["domain"] = "Foreign-UG"
        meta["audience"] = "Foreign"
    elif "international" in n and "admissions" in n and "fee" in n:
        meta["domain"] = "NRI-Intl"
        meta["audience"] = "Foreign"
    elif "pg" in n or "mtech" in n or "m.tech" in n or "mca" in n or "msc" in n or "m.sc" in n:
        meta["domain"] = "PG"
    elif "ug" in n:
        meta["domain"] = "UG"

    # --- Program (PG/UG specifics) ---
    if "mca" in n:
        meta["program"] = "MCA"
    elif "mtech" in n or "m.tech" in n:
        meta["program"] = "M.Tech"
    elif "msc" in n or "m.sc" in n:
        meta["program"] = "M.Sc"
    elif "btech" in n or "b.tech" in n:
        meta["program"] = "B.Tech"

    # --- Hostel attributes ---
    if n.startswith("lh") or "ladies" in n or "girls" in n or "women" in n:
        meta["gender"] = "Female"
    if n.startswith("mh") or "mens" in n or "men's" in n or "boys" in n:
        meta["gender"] = "Male"
    if "first-year" in n or "first year" in n or "freshers" in n or "1st-year" in n:
        meta["student_year"] = "First-Year"
    if "senior" in n:
        meta["student_year"] = "Senior"

    # --- Category ---
    if "refund" in n:
        meta["category"] = "Refund Policy"
        meta["stable_flag"] = "volatile"
    elif "affidavit" in n:
        meta["category"] = "Documents Required"
    elif "document" in n or "submission" in n or "downloads" in n:
        meta["category"] = "Documents Required"
    elif "fee" in n and "hostel" in n:
        meta["category"] = "Fee Structure"
        meta["stable_flag"] = "volatile"
    elif "fee" in n:
        meta["category"] = "Fee Structure"
        meta["stable_flag"] = "volatile"
    elif "faq" in n:
        meta["category"] = "FAQ"
    elif "process" in n or "admissions process" in n or "procedure" in n:
        meta["category"] = "Admission Process"
    elif "programme" in n or "programmes" in n or "courses" in n or "offered" in n:
        meta["category"] = "Courses Offered"
    elif "eligibility" in n:
        meta["category"] = "Eligibility Criteria"
    elif "freshers-hostel-admission-information" in n or ("freshers" in n and "hostel" in n):
        meta["category"] = "Hostel Norms"

    # --- Currency / Audience Hints ---
    if any(k in n for k in ["nri", "foreign", "international"]):
        meta["currency"] = "USD"
    # Default to INR where hostel/tuition but no USD cue
    if meta["category"] == "Fee Structure" and not meta.get("currency"):
        meta["currency"] = "INR"

    # Exam patterns are usually volatile (VITREE/VITEEE/VITMEE)
    if meta["domain"] == "Research" and meta.get("category") in (None, "Admission Process"):
        # leave category for content-based detection later if needed
        pass

    return meta


# ------------------------------ Extractors --------------------------------- #
def extract_pdf_text(path: Path) -> str:
    try:
        with pdfplumber.open(str(path)) as pdf:
            pages = []
            for p in pdf.pages:
                t = p.extract_text() or ""
                pages.append(t)
        return "\n\n".join(pages)
    except Exception as e:
        print(f"[WARN] pdfplumber failed on {path.name}: {e}")
        return ""

def extract_docx_text(path: Path) -> str:
    if not HAS_DOCX:
        return ""
    try:
        d = docx.Document(str(path))
        return "\n".join(p.text for p in d.paragraphs)
    except Exception as e:
        print(f"[WARN] python-docx failed on {path.name}: {e}")
        return ""


# ----------------------------- Ingestion Core ------------------------------ #
def chunk_text(text: str) -> List[str]:
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=450,
        chunk_overlap=60,
        separators=["\n\n", "\n", ". ", " "],
        length_function=len,
    )
    return splitter.split_text(text)

def sha1(s: str) -> str:
    return hashlib.sha1(s.encode("utf-8", errors="ignore")).hexdigest()

def ingest_dir(in_dir: Path, out_jsonl: Path, out_catalog: Path) -> None:
    out_jsonl.parent.mkdir(parents=True, exist_ok=True)
    out_catalog.parent.mkdir(parents=True, exist_ok=True)

    files: List[Path] = []
    for ext in ("*.pdf", "*.PDF", "*.docx", "*.DOCX"):
        files.extend(list(in_dir.rglob(ext)))
    if not files:
        print(f"[ERROR] No PDFs/DOCX found in {in_dir}.")
        sys.exit(1)

    # Catalog headers
    catalog_rows = [["source_file", "size_bytes", "modified_utc", "pages_or_paras",
                     "domain", "category", "program", "gender", "student_year",
                     "audience", "currency", "ay", "stable_flag", "chunk_count"]]

    total_chunks = 0
    with out_jsonl.open("w", encoding="utf-8") as fout:
        for path in tqdm(files, desc="Ingesting"):
            meta = map_metadata(path.name)
            # extract
            if path.suffix.lower() == ".pdf":
                raw = extract_pdf_text(path)
            else:
                raw = extract_docx_text(path)

            cleaned = clean_text(raw)
            if not cleaned.strip():
                print(f"[WARN] Empty after extract/clean: {path.name}")
                continue

            chunks = chunk_text(cleaned)
            mtime = datetime.datetime.utcfromtimestamp(path.stat().st_mtime).strftime("%Y-%m-%d")

            # write chunks
            for i, ch in enumerate(chunks):
                rec = {
                    "id": f"{path.stem}__{i}",
                    "text": ch,
                    "metadata": {
                        **meta,
                        "source_file": path.name,
                        "source_path": str(path),
                        "source_title": path.stem,
                        "chunk_id": i,
                        "last_updated": mtime,
                        "content_hash": sha1(ch),
                    },
                }
                fout.write(json.dumps(rec, ensure_ascii=False) + "\n")

            # catalog row
            pages_or_paras = cleaned.count("\n\n") + 1
            catalog_rows.append([
                path.name,
                path.stat().st_size,
                mtime,
                pages_or_paras,
                meta.get("domain"),
                meta.get("category"),
                meta.get("program"),
                meta.get("gender"),
                meta.get("student_year"),
                meta.get("audience"),
                meta.get("currency"),
                meta.get("ay"),
                meta.get("stable_flag"),
                len(chunks),
            ])

            total_chunks += len(chunks)

    with out_catalog.open("w", newline="", encoding="utf-8") as cf:
        csv.writer(cf).writerows(catalog_rows)

    print(f"[DONE] Wrote {total_chunks} chunks → {out_jsonl}")
    print(f"[DONE] Wrote catalog → {out_catalog}")


# --------------------------------- CLI ------------------------------------- #
def main():
    p = argparse.ArgumentParser(description="Ingest VIT PDFs/DOCX → cleaned chunks JSONL")
    p.add_argument("--in", dest="in_dir", default="data/raw", help="Input folder with PDFs/DOCX")
    p.add_argument("--out", dest="out_jsonl", default="data/processed/chunks.jsonl", help="Output JSONL path")
    p.add_argument("--catalog", dest="catalog", default="data/processed/catalog.csv", help="Catalog CSV path")
    args = p.parse_args()

    in_dir = Path(args.in_dir)
    out_jsonl = Path(args.out_jsonl)
    out_catalog = Path(args.catalog)

    ingest_dir(in_dir, out_jsonl, out_catalog)

if __name__ == "__main__":
    main()
ABOVE IS DATA INGESTION

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
One-pass ingest → clean → sanitize → chunk → dedup → FINAL JSONL
for the VIT Smart FAQ (Vellore MVP).

USAGE:
  pip install "langchain<0.3" pdfplumber python-docx scikit-learn tqdm
  python ingest_clean_finalize.py --in data/raw \
                                  --out data/processed/chunks_clean_final.jsonl \
                                  --catalog data/processed/clean_catalog.csv \
                                  --stats data/processed/clean_stats.txt
"""

import os, re, csv, json, argparse, hashlib, datetime, sys, unicodedata
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from collections import defaultdict

import numpy as np
from tqdm import tqdm

# PDF & DOCX
import pdfplumber
try:
    import docx  # python-docx
    HAS_DOCX = True
except Exception:
    HAS_DOCX = False

# Chunking
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Dedup
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


# ------------------------------ Heuristics ---------------------------------- #

SECTION_HINTS = {
    "Fee Structure": [
        "fee structure", "tuition", "fees", "charges", "payment", "category 1", "category 2",
        "admission fee", "caution deposit", "mess", "special mess", "non-veg", "non veg"
    ],
    "Eligibility Criteria": [
        "eligibility", "minimum marks", "pcm", "pcb", "aggregate", "criteria", "age limit"
    ],
    "Documents Required": [
        "documents required", "affidavit", "undertaking", "certificate", "photograph",
        "marksheet", "reporting", "upload", "submission"
    ],
    "Courses Offered": [
        "programmes offered", "programs offered", "branches", "specialization", "schools", "department"
    ],
    "Academic Rules": [
        "attendance", "grading", "grade point", "ffcs", "regulations", "cgpa"
    ],
    "Hostel Norms": [
        "hostel rules", "norms", "biometric", "in/out", "gate", "appliances", "cooking", "timings"
    ],
    "Refund Policy": [
        "refund", "withdrawal", "vacate", "deduction", "refund policy"
    ],
    "Contacts": [
        "contact", "email", "helpline", "phone", "reach out", "support"
    ],
    "Admission Process": [
        "process", "procedure", "application", "counselling", "slot booking", "steps"
    ],
    "Exam Pattern": [
        "exam pattern", "duration", "sections", "marks", "syllabus", "mcq"
    ],
    "FAQ": [
        "frequently asked questions", "faqs", "question", "answer"
    ],
}

AY_PAT     = re.compile(r'(20\d{2})\D{0,3}(\d{2})')  # 2025-26, 2025–26, 2025_26
EMAIL_PAT  = re.compile(r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}')
PHONE_PAT  = re.compile(r'(?:(?:\+?\d{1,3}[\s-]?)?\d{10,})')

PAGE_PATTERNS = [
    r"Page\s+\d+\s*(?:of|/)\s*\d+",
    r"^\s*Vellore\s+Institute\s+of\s+Technology.*$",
]
PAGE_REGEXES = [re.compile(p, re.IGNORECASE | re.MULTILINE) for p in PAGE_PATTERNS]

# Replacement map for common Unicode punctuation/whitespace weirdness
REPLACEMENTS = {
    "\u2018": "'", "\u2019": "'", "\u201A": "'", "\u201B": "'",
    "\u201C": '"', "\u201D": '"', "\u201E": '"',
    "\u2013": "-", "\u2014": "-", "\u2212": "-",   # en/em dash, minus
    "\u00A0": " ",                                  # NBSP
    "\uFFFD": "",                                   # replacement char
}

# Bullets, arrows, dingbats (often from Office/PUA fonts)
ARROW_BULLET_CHARS = [
    "•","●","◦","∙","·","▪","▫","◾","◽","■","□",
    "➤","➔","➜","➝","➞","➟","➡","➠","→","⇒",
    "►","▸","▹","▶","▷","❯","❱","❭",
    "","","","","","","","","","","",
]
ARROW_BULLET_SET = set(ARROW_BULLET_CHARS)


# ------------------------------ Utilities ----------------------------------- #

def sha1(s: str) -> str:
    return hashlib.sha1(s.encode("utf-8", errors="ignore")).hexdigest()

def guess_ay_from_name(name: str) -> Optional[str]:
    name = name.replace("_", "-")
    m = AY_PAT.search(name)
    if m:
        return f"{m.group(1)}-{m.group(2)}"
    return None

def sanitize_text(s: str) -> str:
    """Unicode normalize + remove PUA/controls + normalize punctuation + map bullets/arrows."""
    if not s:
        return s
    s = unicodedata.normalize("NFKC", s)
    for k, v in REPLACEMENTS.items():
        s = s.replace(k, v)
    # remove (cid:###) artifacts
    s = re.sub(r"\(cid:\d+\)", "", s)
    # map bullets/arrows/checkmarks to "- "
    s = "".join(("- " if ch in ARROW_BULLET_SET else ch) for ch in s)

    # strip other control/private-use chars (keep \n and \t)
    def _ok(ch: str) -> bool:
        cat = unicodedata.category(ch)
        return not (cat and cat[0] == "C") or ch in ("\n", "\t")
    s = "".join(ch for ch in s if _ok(ch))

    # collapse sequences like "--  - -" -> " - "
    s = re.sub(r"(?:\s*-\s*){2,}", " - ", s)
    # tidy whitespace/newlines
    s = re.sub(r"[ \t]{2,}", " ", s)
    s = re.sub(r"\s+\n", "\n", s)
    s = re.sub(r"\n\s+", "\n", s)
    s = re.sub(r"\n{3,}", "\n\n", s)
    # limit repeated punctuation
    s = re.sub(r'([.!?])\1{2,}', r'\1\1', s)   # "!!!!!" -> "!!"
    s = re.sub(r'(,){2,}', r'\1', s)           # ",,,,"  -> ","
    s = re.sub(r'(-){3,}', r'--', s)           # "-----" -> "--"
    # trim stray punctuation at edges
    s = re.sub(r'^[\s\-\.,;:]+', '', s)
    s = re.sub(r'[\s\-\.,;:]+$', '', s)
    return s.strip()

def clean_text_basic(raw: str) -> str:
    """Remove obvious page artifacts, keep paragraphs but join single line-breaks."""
    if not raw:
        return ""
    txt = raw.replace("\r\n", "\n").replace("\r", "\n")
    for rgx in PAGE_REGEXES:
        txt = rgx.sub("", txt)
    txt = re.sub(r"^\s*\d+\s*$", "", txt, flags=re.MULTILINE)   # standalone page numbers
    txt = re.sub(r"(\w)-\n(\w)", r"\1\2", txt)                  # hyphens across lines
    txt = re.sub(r"(?<!\n)\n(?!\n)", " ", txt)                  # join single newlines
    txt = re.sub(r"\n{3,}", "\n\n", txt)                        # keep paragraph breaks
    txt = re.sub(r"[ \t]{2,}", " ", txt)
    return txt.strip()

def extract_pdf_text(path: Path) -> str:
    try:
        with pdfplumber.open(str(path)) as pdf:
            pages = [(p.extract_text() or "") for p in pdf.pages]
        return "\n\n".join(pages)
    except Exception as e:
        print(f"[WARN] pdfplumber failed on {path.name}: {e}")
        return ""

def extract_docx_text(path: Path) -> str:
    if not HAS_DOCX:
        return ""
    try:
        d = docx.Document(str(path))
        return "\n".join(p.text for p in d.paragraphs)
    except Exception as e:
        print(f"[WARN] python-docx failed on {path.name}: {e}")
        return ""

def map_metadata_from_path(path: Path) -> Dict[str, Any]:
    n = path.name.lower()
    parts = [p.lower() for p in path.parts]
    meta: Dict[str, Any] = {
        "campus": "Vellore",
        "domain": None,          # UG | PG | Hostel | Research | ...
        "category": None,        # Fee Structure | Documents Required | ...
        "program": None,         # MCA | M.Tech | M.Sc | B.Tech | Ph.D.
        "gender": None,          # Male | Female (hostel)
        "student_year": None,    # First-Year | Senior (hostel)
        "currency": None,        # INR | USD
        "audience": None,        # NRI | Foreign
        "ay": guess_ay_from_name(n),
        "stable_flag": "stable",
    }
    # domain
    if any("hostel" in p for p in parts) or n.startswith(("mh", "lh")):
        meta["domain"] = "Hostel"
    elif any("research" in p for p in parts) or "vitree" in n or "phd" in n or "ph.d" in n:
        meta["domain"] = "Research"
    elif any("pg" == p or "postgrad" in p for p in parts) or any(k in n for k in ["mtech","m.tech","mca","msc","m.sc"]):
        meta["domain"] = "PG"
    elif any("ug" == p for p in parts) or "ug" in n:
        meta["domain"] = "UG"
    # audience
    if "nri" in n and "ug" in n:
        meta["audience"] = "NRI"
    if "foreign" in n and "ug" in n:
        meta["audience"] = "Foreign"
    if "international" in n and "admissions" in n and "fee" in n:
        meta["domain"] = meta["domain"] or "NRI-Intl"
        meta["audience"] = "Foreign"
    # program
    if "mca" in n:
        meta["program"] = "MCA"
    elif "mtech" in n or "m.tech" in n:
        meta["program"] = "M.Tech"
    elif "msc" in n or "m.sc" in n:
        meta["program"] = "M.Sc"
    elif "btech" in n or "b.tech" in n:
        meta["program"] = "B.Tech"
    elif meta["domain"] == "Research":
        meta["program"] = "Ph.D."
    # hostel attrs
    if n.startswith("lh") or "ladies" in n or "girls" in n or "women" in n:
        meta["gender"] = "Female"
    if n.startswith("mh") or "mens" in n or "men's" in n or "boys" in n:
        meta["gender"] = "Male"
    if any(k in n for k in ["first-year","first year","freshers","1st-year"]):
        meta["student_year"] = "First-Year"
    if "senior" in n:
        meta["student_year"] = "Senior"
    # category
    if "refund" in n:
        meta["category"] = "Refund Policy"; meta["stable_flag"] = "volatile"
    elif "affidavit" in n:
        meta["category"] = "Documents Required"
    elif any(k in n for k in ["document","submission","download"]):
        meta["category"] = "Documents Required"
    elif "fee" in n and "hostel" in n:
        meta["category"] = "Fee Structure"; meta["stable_flag"] = "volatile"
    elif "fee" in n:
        meta["category"] = "Fee Structure"; meta["stable_flag"] = "volatile"
    elif "faq" in n:
        meta["category"] = "FAQ"
    elif any(k in n for k in ["process","procedure","admissions process"]):
        meta["category"] = "Admission Process"
    elif any(k in n for k in ["programme","programmes","courses","offered"]):
        meta["category"] = "Courses Offered"
    elif "eligibility" in n:
        meta["category"] = "Eligibility Criteria"
    elif "freshers-hostel-admission-information" in n or ("freshers" in n and "hostel" in n):
        meta["category"] = "Hostel Norms"
    # currency
    if any(k in n for k in ["nri","foreign","international"]):
        meta["currency"] = "USD"
    if meta["category"] == "Fee Structure" and not meta.get("currency"):
        meta["currency"] = "INR"
    return meta

def refine_category_from_text(text: str, current: Optional[str]) -> Optional[str]:
    t = text.lower()
    scores = {cat: sum(kw in t for kw in keys) for cat, keys in SECTION_HINTS.items()}
    best = max(scores, key=scores.get)
    return best if scores[best] >= 2 else current

def infer_ay_from_text(text: str, current: Optional[str]) -> Optional[str]:
    if current: return current
    m = AY_PAT.search(text)
    return f"{m.group(1)}-{m.group(2)}" if m else current

def extract_contacts(text: str) -> Tuple[List[str], List[str]]:
    emails = sorted(set(EMAIL_PAT.findall(text)))
    phones = sorted(set(PHONE_PAT.findall(text)))
    return emails, phones

def chunk_text(text: str) -> List[str]:
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=450, chunk_overlap=60,
        separators=["\n\n", "\n", ". ", " "],
        length_function=len,
    )
    return splitter.split_text(text)

def merge_small_chunks(records: List[Dict[str, Any]], min_len=280, max_len=520) -> List[Dict[str, Any]]:
    out = []; buf = None
    for rec in records:
        key = (rec["metadata"].get("source_file"),
               rec["metadata"].get("domain"),
               rec["metadata"].get("category"),
               rec["metadata"].get("program"))
        if buf is None:
            buf = rec; continue
        buf_key = (buf["metadata"].get("source_file"),
                   buf["metadata"].get("domain"),
                   buf["metadata"].get("category"),
                   buf["metadata"].get("program"))
        if len(buf["text"]) < min_len and key == buf_key:
            merged_text = (buf["text"].rstrip() + "\n\n" + rec["text"].lstrip()).strip()
            if len(merged_text) <= max_len:
                buf["text"] = merged_text
                continue
        out.append(buf); buf = rec
    if buf is not None:
        out.append(buf)
    return out

def near_dedup(records: List[Dict[str, Any]], threshold=0.97) -> Tuple[List[Dict[str, Any]], int]:
    nrecs = len(records)
    if nrecs < 2:
        return records, 0
    texts = [r["text"] for r in records]
    vec = TfidfVectorizer(max_features=30000, ngram_range=(1,2))
    X = vec.fit_transform(texts)
    if X.shape[1] == 0:
        return records, 0
    keep = []; killed = set(); n = X.shape[0]
    for i in range(n):
        if i in killed: continue
        keep.append(i)
        if i + 1 >= n: continue
        tail = X[i+1:]
        if tail.shape[0] == 0: continue
        sims = cosine_similarity(X[i], tail).ravel()
        for j_off, sim in enumerate(sims, start=1):
            if sim >= threshold:
                killed.add(i + j_off)
    result = [records[idx] for idx in range(n) if idx in keep and idx not in killed]
    return result, len(killed)


# ------------------------------ Main ---------------------------------------- #

def main():
    ap = argparse.ArgumentParser(description="Ingest + sanitize + chunk + dedup to FINAL JSONL")
    ap.add_argument("--in", dest="in_dir", default="data/raw", help="Input folder (recursive)")
    ap.add_argument("--out", dest="out_jsonl", default="data/processed/chunks_clean_final.jsonl",
                    help="Output FINAL cleaned JSONL")
    ap.add_argument("--catalog", dest="catalog_csv", default="data/processed/clean_catalog.csv",
                    help="Output catalog CSV per-file")
    ap.add_argument("--stats", dest="stats_path", default="", help="Optional stats .txt (glyphs before/after)")
    args = ap.parse_args()

    in_dir = Path(args.in_dir).expanduser().resolve()
    out_jsonl = Path(args.out_jsonl).expanduser().resolve()
    catalog_csv = Path(args.catalog_csv).expanduser().resolve()
    out_jsonl.parent.mkdir(parents=True, exist_ok=True)
    catalog_csv.parent.mkdir(parents=True, exist_ok=True)

    print(f"[INFO] Scanning input folder: {in_dir}")
    print(f"[INFO] Writing FINAL chunks : {out_jsonl}")
    print(f"[INFO] Writing catalog      : {catalog_csv}")

    files: List[Path] = []
    for ext in ("*.pdf", "*.PDF", "*.docx", "*.DOCX"):
        files.extend(in_dir.rglob(ext))
    if not files:
        print(f"[ERROR] No PDFs/DOCX found under: {in_dir}"); sys.exit(1)

    file_reports = []
    all_cleaned: List[Dict[str, Any]] = []

    # stats for glyphs before/after
    def has_weird(s: str) -> bool:
        if not s: return False
        return any(ch in s for ch in ARROW_BULLET_SET) or "(cid:" in s or "\uFFFD" in s
    glyphs_before = 0
    glyphs_after  = 0
    total_chunks  = 0

    for path in tqdm(files, desc="Processing files"):
        meta_base = map_metadata_from_path(path)

        # Extract → basic clean → sanitize once globally
        raw = extract_pdf_text(path) if path.suffix.lower() == ".pdf" else extract_docx_text(path)
        cleaned_basic = clean_text_basic(raw)
        cleaned = sanitize_text(cleaned_basic)

        if not cleaned.strip():
            print(f"[WARN] Empty after extract/clean: {path.name}")
            file_reports.append([path.name, 0, 0, 0, "EMPTY"])
            continue

        # Chunk
        chunks = chunk_text(cleaned)
        total_chunks += len(chunks)

        # Build per-chunk records
        recs = []
        mtime = datetime.datetime.utcfromtimestamp(path.stat().st_mtime).strftime("%Y-%m-%d")
        for i, ch in enumerate(chunks):
            # extra per-chunk sanitize (idempotent)
            if has_weird(ch): glyphs_before += 1
            ch2 = sanitize_text(ch)
            if has_weird(ch2): glyphs_after += 1
            recs.append({
                "id": f"{path.stem}__{i}",
                "text": ch2,
                "metadata": {
                    **meta_base,
                    "source_file": path.name,
                    "source_path": str(path),
                    "source_title": path.stem,
                    "chunk_id": i,
                    "last_updated": mtime,
                    "content_hash": sha1(ch2),
                }
            })

        # Per-chunk enrich (category/AY/contacts)
        refined_cats = 0; inferred_ays = 0; contacts_enriched = 0
        for r in recs:
            new_cat = refine_category_from_text(r["text"], r["metadata"].get("category"))
            if new_cat != r["metadata"].get("category"):
                r["metadata"]["category"] = new_cat; refined_cats += 1
            new_ay = infer_ay_from_text(r["text"], r["metadata"].get("ay"))
            if new_ay and new_ay != r["metadata"].get("ay"):
                r["metadata"]["ay"] = new_ay; inferred_ays += 1
            emails, phones = extract_contacts(r["text"])
            if emails or phones:
                r["metadata"]["emails"] = emails
                r["metadata"]["phones"] = phones
                contacts_enriched += 1

        # Merge tiny chunks & dedup
        recs.sort(key=lambda x: x["metadata"].get("chunk_id", 0))
        merged = merge_small_chunks(recs, min_len=280, max_len=520)
        if len(merged) >= 2:
            deduped, killed = near_dedup(merged, threshold=0.97)
        else:
            deduped, killed = merged, 0

        # Exact dedup inside file
        seen = set(); unique = []
        for r in deduped:
            h2 = sha1(r["text"])
            if h2 in seen: continue
            seen.add(h2); r["metadata"]["content_hash2"] = h2; unique.append(r)

        all_cleaned.extend(unique)

        file_reports.append([
            path.name, len(chunks), len(merged), len(unique),
            f"refined_cat:{refined_cats}|inferred_ay:{inferred_ays}|contacts:{contacts_enriched}|near_killed:{killed}"
        ])

    # Global exact dedup
    global_seen = set(); final_records: List[Dict[str, Any]] = []
    for r in all_cleaned:
        h = sha1(r["text"])
        if h in global_seen: continue
        global_seen.add(h); final_records.append(r)

    # Write outputs
    with out_jsonl.open("w", encoding="utf-8") as f:
        for r in final_records:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")

    with catalog_csv.open("w", newline="", encoding="utf-8") as cf:
        w = csv.writer(cf)
        w.writerow(["source_file","chunks_raw","chunks_after_merge","chunks_final","notes"])
        for row in file_reports:
            w.writerow(row)

    # Optional stats
    if args.stats_path:
        sp = Path(args.stats_path)
        sp.parent.mkdir(parents=True, exist_ok=True)
        with sp.open("w", encoding="utf-8") as sf:
            sf.write(f"total_input_chunks: {total_chunks}\n")
            sf.write(f"glyphs_before:      {glyphs_before}\n")
            sf.write(f"glyphs_after:       {glyphs_after}\n")
            sf.write(f"final_chunks:       {len(final_records)}\n")
        print(f"[INFO] wrote stats → {sp}")

    print(f"[DONE] Files processed : {len(files)}")
    print(f"[DONE] Final chunks    : {len(final_records)} → {out_jsonl}")
    print(f"[DONE] Clean catalog   : {catalog_csv}")
    print("[TIP ] Next: use this FINAL JSONL for embeddings & indexing.")

# --------- Optional: simple keyword→filter helper for your router ---------- #
DOMAIN_MAP = {
    "ug":"UG", "undergrad":"UG",
    "pg":"PG", "postgrad":"PG", "post graduate":"PG", "post-graduate":"PG",
    "hostel":"Hostel", "accommodation":"Hostel",
    "research":"Research", "phd":"Research", "vitree":"Research",
}
PROGRAM_MAP = {
    "mca":"MCA", "m.tech":"M.Tech", "mtech":"M.Tech", "msc":"M.Sc", "m.sc":"M.Sc",
    "btech":"B.Tech","b.tech":"B.Tech","phd":"Ph.D"
}
CATEGORY_MAP = {
    "refund policy":"Refund Policy", "refund":"Refund Policy",
    "fee structure":"Fee Structure", "fees":"Fee Structure", "tuition":"Fee Structure",
    "courses offered":"Courses Offered", "programs offered":"Courses Offered", "programmes offered":"Courses Offered",
    "eligibility criteria":"Eligibility Criteria", "eligibility":"Eligibility Criteria",
    "documents required":"Documents Required", "documents":"Documents Required", "document submission":"Documents Required",
    "academic rules":"Academic Rules", "attendance":"Academic Rules", "grading":"Academic Rules", "ffcs":"Academic Rules",
    "hostel norms":"Hostel Norms", "norms":"Hostel Norms",
    "admission process":"Admission Process", "process":"Admission Process", "procedure":"Admission Process",
    "exam pattern":"Exam Pattern", "faq":"FAQ", "faqs":"FAQ",
    "contacts":"Contacts", "contact":"Contacts", "helpline":"Contacts"
}
AUDIENCE_MAP = {
    "nri":"NRI", "foreign":"Foreign", "international":"Foreign"
}
def build_filters_from_query(q: str) -> Dict[str, Any]:
    ql = q.lower()
    where: Dict[str, Any] = {}
    for k,v in DOMAIN_MAP.items():
        if k in ql: where["domain"] = v; break
    for k,v in PROGRAM_MAP.items():
        if k in ql: where["program"] = v; break
    for k,v in CATEGORY_MAP.items():
        if k in ql: where["category"] = v; break
    for k,v in AUDIENCE_MAP.items():
        if k in ql: where["audience"] = v; break
    where.setdefault("campus", "Vellore")   # MVP default
    return where


if __name__ == "__main__":
    main()
ABOVE IS PREPROCESSED ONE


#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Build a FAISS index from chunks_clean_final.jsonl
Embeddings: Gemini (default) or Sentence-Transformers (local)

Usage:
  python embeddings_faiss_gemini.py --chunks Data/processed/chunks_clean_final.jsonl ^
                                    --persist Data/index/faiss ^
                                    --collection vit_faq_vellore ^
                                    --emb gemini

  # Or local embeddings (no API):
  python embeddings_faiss_gemini.py --chunks Data/processed/chunks_clean_final.jsonl ^
                                    --persist Data/index/faiss ^
                                    --collection vit_faq_vellore ^
                                    --emb st
"""
import os, sys, json, argparse
from pathlib import Path
from tqdm import tqdm

# Only use the community import (no deprecation warning)
from langchain_community.vectorstores import FAISS

try:
    from dotenv import load_dotenv
    load_dotenv()
except Exception:
    pass


# ---------------- Embedding backends ---------------- #

class GeminiEmbeddings:
    """
    Minimal adapter for Gemini embeddings.
    Uses the CURRENT model: models/text-embedding-004
    """
    def __init__(self, model: str = "models/text-embedding-004", api_key_env: str = "GEMINI_API_KEY"):
        import google.generativeai as genai
        api_key = os.getenv(api_key_env)
        if not api_key:
            print("[ERROR] GEMINI_API_KEY not set. Set it via environment or .env file.")
            sys.exit(1)
        genai.configure(api_key=api_key)
        self.genai = genai
        self.model = model

    def embed_documents(self, texts):
        out = []
        for t in texts:
            # embed_content returns {'embedding': [...]} for this model
            r = self.genai.embed_content(model=self.model, content=t)
            vec = r.get("embedding")
            if vec is None:
                raise RuntimeError("Gemini embed_content returned no 'embedding'. Check model & quota.")
            out.append(vec)
        return out

    def embed_query(self, q):
        r = self.genai.embed_content(model=self.model, content=q)
        vec = r.get("embedding")
        if vec is None:
            raise RuntimeError("Gemini embed_content returned no 'embedding' for query.")
        return vec


class STEmbeddings:
    """Sentence-Transformers local embeddings (no API)."""
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        try:
            from sentence_transformers import SentenceTransformer
        except Exception as e:
            print("[ERROR] sentence-transformers not installed. pip install sentence-transformers")
            raise
        self.model = SentenceTransformer(model_name)

    def embed_documents(self, texts):
        return self.model.encode(texts, normalize_embeddings=True).tolist()

    def embed_query(self, q):
        return self.model.encode([q], normalize_embeddings=True)[0].tolist()


# ---------------- Utilities ---------------- #

def load_chunks(jsonl_path: Path):
    texts, metas, ids = [], [], []
    with jsonl_path.open("r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue
            obj = json.loads(line)
            txt = (obj.get("text") or "").strip()
            if not txt:
                continue
            texts.append(txt)
            metas.append(obj.get("metadata", {}) or {})
            ids.append(obj.get("id") or f"id_{len(ids)}")
    return texts, metas, ids


# ---------------- Build FAISS ---------------- #

def build_faiss(chunks, persist_dir: Path, collection: str, emb):
    texts, metas, ids = chunks
    if not texts:
        print("[ERROR] No texts to index. Is your chunks file empty?")
        sys.exit(1)

    print(f"[INFO] Building FAISS index with {len(texts)} docs (this may take a minute)...")
    # langchain_community FAISS expects .embed_documents/.embed_query on embedding object
    vectors = emb.embed_documents(texts)

    # Create store from precomputed embeddings to avoid re-embedding inside FAISS
    # (this path avoids issues if your embedding object isn't a LangChain Embeddings subclass)
    from langchain_community.docstore.in_memory import InMemoryDocstore
    from langchain_core.documents import Document
    import faiss
    import numpy as np
    import uuid

    dim = len(vectors[0])
    index = faiss.IndexFlatIP(dim)  # cosine similarity via normalized vectors (Gemini returns L2; we keep IP; normalize if needed)
    # normalize embeddings for cosine similarity
    vec_np = np.array(vectors, dtype="float32")
    norms = np.linalg.norm(vec_np, axis=1, keepdims=True) + 1e-12
    vec_np = vec_np / norms
    index.add(vec_np)

    # Build documents & ID map
    docs = {}
    id_map = {}
    for i, (t, m) in enumerate(zip(texts, metas)):
        did = ids[i] if i < len(ids) else str(uuid.uuid4())
        docs[did] = Document(page_content=t, metadata=m)
        id_map[i] = did

    store = FAISS(
        embedding_function=emb,  # kept for API compatibility; not used since we precomputed
        index=index,
        docstore=InMemoryDocstore(docs),
        index_to_docstore_id=id_map,
    )

    persist_dir.mkdir(parents=True, exist_ok=True)
    store.save_local(str(persist_dir), index_name=collection)
    print(f"[DONE] FAISS saved at: {persist_dir}/{collection}.faiss (+ .pkl)")


# ---------------- Main ---------------- #

if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--chunks", required=True, help="Path to Data/processed/chunks_clean_final.jsonl")
    ap.add_argument("--persist", required=True, help="Folder to save FAISS (e.g., Data/index/faiss)")
    ap.add_argument("--collection", default="vit_faq_vellore", help="Index name")
    ap.add_argument("--emb", choices=["gemini","st"], default="gemini", help="Embedding backend")
    args = ap.parse_args()

    chunks_path = Path(args.chunks).resolve()
    if not chunks_path.exists():
        print(f"[ERROR] chunks not found: {chunks_path}")
        sys.exit(1)

    if args.emb == "gemini":
        emb = GeminiEmbeddings()  # uses models/text-embedding-004
    else:
        emb = STEmbeddings()

    texts, metas, ids = load_chunks(chunks_path)
    build_faiss((texts, metas, ids), Path(args.persist).resolve(), args.collection, emb)
ABOVE ONE IS EMBEDDINGS

# ETL/load_sqlite.py
# Build SQLite DB from hostel staging CSVs + curated academics CSVs (NO VITREE/RULES)
# Output: Data/sql/vit_vellore.db

import sqlite3, pathlib, csv, re, os, glob
from typing import Optional, Tuple, List
import pandas as pd

# -------------------- Paths --------------------
BASE     = pathlib.Path("Data")
STAGING  = BASE / "staging"        # auto-staged CSVs from PDFs (hostel + misc)
PROCESSED= BASE / "processed"      # curated CSVs (source of truth for academics)
SQLDIR   = BASE / "sql"
SQLDIR.mkdir(parents=True, exist_ok=True)
DB_PATH  = SQLDIR / "vit_vellore.db"

MONEY = r"(?:₹|INR|USD|\$)\s*[\d,]+(?:\.\d+)?"

# -------------------- DB utils --------------------
def _mk_conn():
    con = sqlite3.connect(DB_PATH)
    con.execute("PRAGMA journal_mode=WAL;")
    con.execute("PRAGMA synchronous=NORMAL;")
    con.row_factory = sqlite3.Row
    return con

def _exec_schema(con: sqlite3.Connection, ddl: str):
    con.executescript(ddl)

def _schema(con: sqlite3.Connection):
    """Create HOSTEL + ACADEMICS schema (no VITREE/RULES)."""
    _exec_schema(con, """
    -- ---------------- HOSTEL ----------------
    CREATE TABLE IF NOT EXISTS blocks (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        block_name   TEXT,
        display_name TEXT,
        gender       TEXT,     -- Male | Female
        level        TEXT,     -- First-Year | Senior | NULL
        block_type   TEXT
    );

    CREATE TABLE IF NOT EXISTS hostel_fees (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        block_id INTEGER NOT NULL,
        ay TEXT,                  -- 2025-26
        category TEXT,            -- Indian | NRI | Foreign | CAT1 | CAT2 (if any)
        occupancy TEXT,           -- "2 Sharing" / "6 Sharing" / "2 Bed" etc
        ac INTEGER,               -- 1=AC, 0=Non-AC, NULL=unknown
        mess_type TEXT,           -- Special / Non-Veg / Veg / ...
        room_mess_fee TEXT,
        admission_fee TEXT,
        caution_deposit TEXT,
        other_fee TEXT,
        total_fee TEXT,
        currency TEXT,            -- INR | USD
        source_file TEXT
    );

    CREATE TABLE IF NOT EXISTS amenities (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        block_id INTEGER NOT NULL,
        key   TEXT,
        value TEXT
    );

    CREATE TABLE IF NOT EXISTS contacts (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        block_id INTEGER NOT NULL,
        name  TEXT,
        role  TEXT,
        phone TEXT,
        email TEXT
    );

    -- lightweight extracted helper tables from Hostel_info
    CREATE TABLE IF NOT EXISTS mh_blocks (
        block_code TEXT,
        block_name TEXT,
        email     TEXT,
        landline  TEXT
    );
    CREATE TABLE IF NOT EXISTS lh_blocks (
        block_code TEXT,
        landline   TEXT
    );
    CREATE TABLE IF NOT EXISTS hostel_contacts (
        role  TEXT,
        name  TEXT,
        email TEXT,
        phone TEXT
    );

    -- helpful hostel indexes
    CREATE INDEX IF NOT EXISTS idx_blocks_name   ON blocks(block_name);
    CREATE INDEX IF NOT EXISTS idx_hostel_fees   ON hostel_fees(block_id, ay, category);

    -- ---------------- ACADEMICS ----------------
    CREATE TABLE IF NOT EXISTS programs (
      id INTEGER PRIMARY KEY AUTOINCREMENT,
      level TEXT,                  -- UG | PG | MCA | MSc
      program TEXT,                -- e.g., "B.Tech CSE"
      school TEXT,                 -- optional
      duration TEXT,               -- e.g., "4 years"
      campus TEXT,                 -- optional
      source_file TEXT
    );

    CREATE TABLE IF NOT EXISTS eligibility (
      id INTEGER PRIMARY KEY AUTOINCREMENT,
      level TEXT,
      program TEXT,                -- "ALL" for common/overall level criteria
      criteria TEXT,
      source_file TEXT
    );

    CREATE TABLE IF NOT EXISTS documents_required (
      id INTEGER PRIMARY KEY AUTOINCREMENT,
      level TEXT,
      program TEXT,                -- "ALL" for level-wide docs
      item TEXT,
      details TEXT,
      source_file TEXT
    );

    CREATE TABLE IF NOT EXISTS academic_fees (
      id INTEGER PRIMARY KEY AUTOINCREMENT,
      level TEXT,
      program TEXT,                -- "ALL" when fees are not per-program
      category TEXT,               -- Indian | NRI | Foreign | CAT1 | CAT2 etc.
      ay TEXT,                     -- 2025-26
      tuition TEXT,
      one_time TEXT,
      caution TEXT,
      total TEXT,
      currency TEXT,               -- INR | USD
      source_file TEXT
    );

    CREATE TABLE IF NOT EXISTS scholarships (
      id INTEGER PRIMARY KEY AUTOINCREMENT,
      level TEXT,                  -- UG | PG | Research | etc.
      name TEXT,
      criteria TEXT,
      amount TEXT,
      currency TEXT,
      source_file TEXT
    );

    -- helpful academics indexes
    CREATE INDEX IF NOT EXISTS idx_prog ON programs(level, program);
    CREATE INDEX IF NOT EXISTS idx_elig ON eligibility(level, program);
    CREATE INDEX IF NOT EXISTS idx_docs ON documents_required(level, program);
    CREATE INDEX IF NOT EXISTS idx_fees ON academic_fees(level, program, category, ay);
    CREATE INDEX IF NOT EXISTS idx_sch  ON scholarships(level, name);
    """)

# -------------------- HOSTEL staging → SQL --------------------
def _get_or_create_block(con: sqlite3.Connection, block_name: str, display: Optional[str],
                         gender: str, level: Optional[str], btype: Optional[str]) -> int:
    row = con.execute(
        "SELECT id FROM blocks WHERE block_name=? AND IFNULL(level,'')=IFNULL(?, '') AND IFNULL(block_type,'')=IFNULL(?, '') AND IFNULL(gender,'')=IFNULL(?, '')",
        (block_name, level, btype, gender)
    ).fetchone()
    if row: return row[0]
    con.execute("INSERT INTO blocks(block_name, display_name, gender, level, block_type) VALUES(?,?,?,?,?)",
                (block_name, display or block_name, gender, level, btype))
    return con.execute("SELECT last_insert_rowid()").fetchone()[0]

def _guess_meta_from_filename(name: str) -> Tuple[str, Optional[str], str, str]:
    """Returns: gender, level, ay, category_hint (hostel-only helper)"""
    n = name.lower()
    gender = "Male" if n.startswith("mh") else ("Female" if n.startswith("lh") else "")
    level  = "Senior" if "senior" in n else ("First-Year" if ("first-year" in n or "first year" in n) else None)
    ay = ""
    m = re.search(r"(20\d{2})\D{0,3}(\d{2})", n)
    if m: ay = f"{m.group(1)}-{m.group(2)}"
    if "nri" in n or "foreign" in n:
        cat = "NRI"
    elif "indian" in n:
        cat = "Indian"
    else:
        cat = ""
    return gender, level, ay, cat

HEADER_ALIASES = {
    "room_type":      ["roomtype", "room", "acnonac", "ac/nonac", "ac", "nonac", "category", "roomcategory"],
    "mess_type":      ["messtype", "diet", "veg", "nonveg", "specialmess", "special"],
    "room_mess_fee":  ["roomandmessfee", "roommessfee", "hostelfee", "room&messfee", "roommess", "roomandmess", "hostelandmessfee"],
    "admission_fee":  ["admissionfee", "admission"],
    "caution_deposit":["cautiondeposit", "refundabledeposit", "caution", "deposit"],
    "other_fee":      ["other", "utility", "electricity", "maintenance", "service"],
    "total_fee":      ["total", "grandtotal", "overalltotal", "nettotal", "totalamount"],
}

def _norm(s: str) -> str:
    return re.sub(r"[^a-z0-9]+","", (s or "").lower())

def _score_header_row(cells: List[str]) -> int:
    toks = [_norm(c) for c in cells]
    score = 0
    for k in "room mess total admission caution deposit fee ac non".split():
        if any(k in t for t in toks): score += 1
    return score

def _find_header_idx(rows: List[List[str]]) -> int:
    best_i, best_s = 0, -1
    for i in range(min(6, len(rows))):
        s = _score_header_row(rows[i])
        if s > best_s:
            best_s, best_i = s, i
    return best_i

def _map_columns(header_cells: List[str]) -> dict:
    hnorm = [_norm(c) for c in header_cells]
    mapping = {k: None for k in HEADER_ALIASES.keys()}
    for k, aliases in HEADER_ALIASES.items():
        for idx, h in enumerate(hnorm):
            if any(a in h for a in aliases):
                if k == "total_fee":
                    mapping[k] = idx
                elif mapping[k] is None:
                    mapping[k] = idx
    return mapping

def _detect_occupancy(text: str) -> Optional[str]:
    m = re.search(r"\b(\d+)\s*(?:/|\s)?\s*(\d+)?\s*(sharing|seater|bed|occupancy)\b", text, flags=re.I) \
        or re.search(r"\b(\d+)\s*(sharing|seater|bed|occupancy)\b", text, flags=re.I)
    if not m: return None
    g = [x for x in m.groups() if x and x.isdigit()]
    word = (m.groups()[-1] or "").title()
    return f"{g[0]}/{g[1]} {word}" if len(g) == 2 else f"{g[0]} {word}"

def _detect_ac(text: str) -> Optional[int]:
    if re.search(r"\bNon[- ]?AC\b", text, flags=re.I): return 0
    if re.search(r"\bAC\b|\bA/C\b", text, flags=re.I): return 1
    return None

def _clean_amt(s: str) -> str:
    s = (s or "").strip()
    return re.sub(r"[^\d₹$,\.INRUSD ]", "", s) if s else ""

def _row_has_any_value(*vals) -> bool:
    return any((v or "").strip() for v in vals)

def _currency_from(filename: str, row_text: str, header_text: str) -> str:
    if "usd" in row_text.lower() or "$" in row_text or "usd" in header_text.lower():
        return "USD"
    if "inr" in row_text.lower() or "₹" in row_text or "inr" in header_text.lower():
        return "INR"
    fn = filename.lower()
    if ("nri" in fn) or ("foreign" in fn): return "USD"
    if ("indian" in fn): return "INR"
    return ""

def load_staging_csvs_hostel(con: sqlite3.Connection):
    files = sorted(STAGING.glob("*.csv"))
    for path in files:
        stem = path.stem.lower()
        # only fee-structure CSVs (leave other CSVs to the academics loader)
        if not (stem.startswith("mh-") or stem.startswith("lh-") or "hostel" in stem):
            continue

        gender, level, ay, cat_hint = _guess_meta_from_filename(path.stem)
        block_title = ("Men Hostel" if gender=="Male" else "Ladies Hostel") + (f" {level}" if level else "")
        block_id = _get_or_create_block(con, block_title, block_title, gender or "", level, None)

        rows = list(csv.reader(path.open("r", encoding="utf-8")))
        if not rows: 
            continue

        hi = _find_header_idx(rows)
        header = rows[hi]
        colmap = _map_columns(header)
        header_text = " ".join(header)

        for r in rows[hi+1:]:
            if not any((c or "").strip() for c in r): 
                continue
            room_col = colmap.get("room_type"); mess_col = colmap.get("mess_type")
            room_txt = (r[room_col] if (room_col is not None and room_col < len(r)) else "") or ""
            mess_txt = (r[mess_col] if (mess_col is not None and mess_col < len(r)) else "") or ""
            joined = " ".join([c for c in r if c])

            occ = _detect_occupancy(room_txt) or _detect_occupancy(joined) or ""
            ac  = _detect_ac(room_txt) or _detect_ac(joined)
            mess = mess_txt.strip()
            if re.fullmatch(MONEY, mess): mess = ""
            if re.search(r"\bspecial\b", mess, re.I): mess = "Special Mess"
            elif re.search(r"non\s*veg", mess, re.I): mess = "Non Veg"
            elif re.search(r"\bveg\b", mess, re.I):   mess = "Veg"

            def pick_amount(key: str) -> str:
                idx = colmap.get(key); return _clean_amt(r[idx]) if (idx is not None and idx < len(r)) else ""

            room_mess  = pick_amount("room_mess_fee")
            admission  = pick_amount("admission_fee")
            caution    = pick_amount("caution_deposit")
            other      = pick_amount("other_fee")
            total      = pick_amount("total_fee")

            row_text = " ".join(r)
            currency = _currency_from(path.name, row_text, header_text)
            category = cat_hint or ("NRI" if currency=="USD" else ("Indian" if currency=="INR" else ""))

            if not _row_has_any_value(room_mess, admission, caution, other, total):
                m_all = re.findall(MONEY, row_text)
                if m_all:
                    room_mess = m_all[0]
                    if len(m_all) > 1:
                        total = m_all[-1]

            con.execute("""
                INSERT INTO hostel_fees(block_id, ay, category, occupancy, ac, mess_type,
                                        room_mess_fee, admission_fee, caution_deposit, other_fee, total_fee, currency, source_file)
                VALUES(?,?,?,?,?,?,?,?,?,?,?,?,?)
            """, (block_id, ay, category, occ, ac, mess, room_mess, admission, caution, other, total, currency, path.name))

    print("[OK] Loaded hostel fee CSVs into SQLITE.")

# ---------- parse Hostel_info into mh_blocks/lh_blocks/hostel_contacts ----------
def _flatten_hostel_info_frames() -> str:
    paths = sorted(glob.glob(os.path.join(str(STAGING), "Hostel_info__camelot_*.csv")))
    if not paths:
        return ""
    frames = []
    for p in paths:
        df = pd.read_csv(p, header=None, dtype=str).fillna("")
        frames.append(df)
    raw = pd.concat(frames, ignore_index=True)
    raw["joined"] = raw.apply(lambda r: " ".join([c for c in r if isinstance(c, str)]), axis=1)
    text = "\n".join(raw["joined"].tolist())
    text = re.sub(r"\s+", " ", text)
    text = text.replace("vit.ac.i n", "vit.ac.in")
    return text

def load_hostel_info(con: sqlite3.Connection):
    text = _flatten_hostel_info_frames()
    if not text:
        print("[WARN] Hostel_info__camelot_*.csv not found in staging; skipping mh/lh blocks + contacts.")
        return

    pat_mh = re.compile(
        r"(MH [A-Z](?: ANNEX)?)\s+([A-Z0-9 .’'--]+?(?:ANNEX)?)(?:\s+-\s*[A-Z ]+)?\s+0416\s*220\s*(\d{4})\s+([A-Za-z.]+@vit\.ac\.in)",
        flags=re.I,
    )
    mh = pd.DataFrame(pat_mh.findall(text), columns=["block_code","block_name","last4","email"])
    if not mh.empty:
        mh["landline"] = "0416 220 " + mh["last4"]
        mh.drop(columns=["last4"], inplace=True)
        mh["block_code"] = mh["block_code"].str.upper().str.strip()
        mh["block_name"] = (mh["block_name"].str.replace("–", "-", regex=False).str.title().str.strip())

    pat_lh = re.compile(r"(LH [A-Z]|RGT H|LH GH \(Annex\))\s+0416\s*220\s*(\d{4})", flags=re.I)
    lh = pd.DataFrame(pat_lh.findall(text), columns=["block_code","last4"])
    if not lh.empty:
        lh["landline"] = "0416 220 " + lh["last4"]
        lh.drop(columns=["last4"], inplace=True)
        lh["block_code"] = lh["block_code"].str.upper().str.strip()

    rows = [
        ("Section Supervisor (MH)", "Mr. Arasu R", "rarasu@vit.ac.in", "0416-220-2523"),
        ("Section Supervisor (LH)", "Ms. G. Subbulakshmi", "gsubbulakshmi@vit.ac.in", "0416-220-2711"),
        ("Residential Block Supervisor (LH, Transport)", "Ms. Mythily A", "mythily.a@vit.ac.in", "9488839864, 9791297375"),
    ]
    contacts = pd.DataFrame(rows, columns=["role","name","email","phone"])

    cur = con.cursor()
    cur.execute("DELETE FROM mh_blocks")
    cur.execute("DELETE FROM lh_blocks")
    cur.execute("DELETE FROM hostel_contacts")

    if not mh.empty:
        mh.to_sql("mh_blocks", con, index=False, if_exists="append")
    if not lh.empty:
        lh.to_sql("lh_blocks", con, index=False, if_exists="append")
    contacts.to_sql("hostel_contacts", con, index=False, if_exists="append")
    print("[OK] Loaded Hostel Info (blocks + contacts).")

# -------------------- ACADEMICS: load from Data/processed/** --------------------
def _append_df(con, table, df):
    df = df.where(pd.notna(df), "")
    df.to_sql(table, con, if_exists="append", index=False)

def _clear_academics(con):
    for t in ["programs","eligibility","documents_required","academic_fees","scholarships"]:
        con.execute(f"DELETE FROM {t}")

def _load_programs_generic(con, path: pathlib.Path, level_hint=None):
    df = pd.read_csv(path, dtype=str).fillna("")
    df.columns = [c.strip().lower() for c in df.columns]
    for c in ["level","program","school","duration","campus","source_file"]:
        if c not in df.columns: df[c] = ""
    if level_hint and (df["level"] == "").all():
        df["level"] = level_hint
    _append_df(con, "programs", df[["level","program","school","duration","campus","source_file"]])

def _load_documents_generic(con, path: pathlib.Path, level_hint=None):
    df = pd.read_csv(path, dtype=str).fillna("")
    df.columns = [c.strip().lower() for c in df.columns]
    rename = {"document":"item","doc":"item","name":"item","description":"details","detail":"details"}
    df = df.rename(columns={k:v for k,v in rename.items() if k in df.columns})
    for c in ["level","program","item","details","source_file"]:
        if c not in df.columns: df[c] = ""
    if level_hint and (df["level"] == "").all(): df["level"] = level_hint
    _append_df(con, "documents_required", df[["level","program","item","details","source_file"]])

def _load_eligibility_generic(con, path: pathlib.Path, level_hint=None):
    df = pd.read_csv(path, dtype=str).fillna("")
    df.columns = [c.strip().lower() for c in df.columns]
    for c in ["level","program","criteria","source_file"]:
        if c not in df.columns: df[c] = ""
    if level_hint and (df["level"] == "").all(): df["level"] = level_hint
    _append_df(con, "eligibility", df[["level","program","criteria","source_file"]])

def _load_scholarships_generic(con, path: pathlib.Path, level_hint=None):
    df = pd.read_csv(path, dtype=str).fillna("")
    df.columns = [c.strip().lower() for c in df.columns]
    for c in ["level","name","criteria","amount","currency","source_file"]:
        if c not in df.columns: df[c] = ""
    if level_hint and (df["level"] == "").all(): df["level"] = level_hint
    _append_df(con, "scholarships", df[["level","name","criteria","amount","currency","source_file"]])

def _load_ug_fees(con, path: pathlib.Path):
    df = pd.read_csv(path, dtype=str).fillna("")
    df.columns = [c.strip().lower().replace(" ","_") for c in df.columns]
    rename = {
        "tuition_fee":"tuition","tuition_fees":"tuition",
        "one_time_fee":"one_time","admission_fee":"one_time","enrolment_fee":"one_time","registration_fee":"one_time",
        "caution_deposit":"caution","caution_deposit_inr":"caution",
        "total_fee":"total","total_inr":"total"
    }
    for k,v in rename.items():
        if k in df.columns and v not in df.columns:
            df = df.rename(columns={k:v})
    for c in ["level","program","category","ay","tuition","one_time","caution","total","currency","source_file"]:
        if c not in df.columns: df[c] = ""
    # infer AY if empty
    if (df["ay"] == "").all():
        df["ay"] = "2025-26"
    _append_df(con, "academic_fees",
               df[["level","program","category","ay","tuition","one_time","caution","total","currency","source_file"]])

def _load_pg_fees(con, path: pathlib.Path):
    """Reshape pg_fees.csv (cat1/cat2 columns) into academic_fees rows."""
    df = pd.read_csv(path, dtype=str).fillna("")
    df.columns = [c.strip().lower() for c in df.columns]
    rows = []
    for _, r in df.iterrows():
        rows.append(dict(level="PG", program=r.get("program",""), category="CAT1", ay="2025-26",
                         tuition=r.get("cat1_tuition_inr",""), one_time="",
                         caution=r.get("caution_deposit_inr",""), total=r.get("total_cat1_inr",""),
                         currency="INR", source_file="pg_fees.csv"))
        rows.append(dict(level="PG", program=r.get("program",""), category="CAT2", ay="2025-26",
                         tuition=r.get("cat2_tuition_inr",""), one_time="",
                         caution=r.get("caution_deposit_inr",""), total=r.get("total_cat2_inr",""),
                         currency="INR", source_file="pg_fees.csv"))
    if rows:
        out = pd.DataFrame(rows)
        _append_df(con, "academic_fees",
                   out[["level","program","category","ay","tuition","one_time","caution","total","currency","source_file"]])

def load_academics_from_processed(con: sqlite3.Connection):
    # UG
    ug = PROCESSED / "UG"
    if ug.exists():
        if (ug/"programs_ug (1).csv").exists(): _load_programs_generic(con, ug/"programs_ug (1).csv", "UG")
        if (ug/"fee_structure_2025_2026.csv").exists(): _load_ug_fees(con, ug/"fee_structure_2025_2026.csv")
        if (ug/"eligibility_2025_2026.csv").exists(): _load_eligibility_generic(con, ug/"eligibility_2025_2026.csv", "UG")
        if (ug/"ug_documents.csv").exists(): _load_documents_generic(con, ug/"ug_documents.csv", "UG")
        if (ug/"scholarships_ug.csv").exists(): _load_scholarships_generic(con, ug/"scholarships_ug.csv", "UG")

    # PG
    pg = PROCESSED / "PG"
    if pg.exists():
        if (pg/"pg_programs.csv").exists(): _load_programs_generic(con, pg/"pg_programs.csv", "PG")
        if (pg/"pg_fees.csv").exists(): _load_pg_fees(con, pg/"pg_fees.csv")
        if (pg/"pg_eligibility.csv").exists(): _load_eligibility_generic(con, pg/"pg_eligibility.csv", "PG")
        if (pg/"pg_documents.csv").exists(): _load_documents_generic(con, pg/"pg_documents.csv", "PG")

    # MCA
    mca = PROCESSED / "MCA"
    if mca.exists():
        if (mca/"mca_programs.csv").exists(): _load_programs_generic(con, mca/"mca_programs.csv", "MCA")
        if (mca/"mca_fees.csv").exists(): _load_ug_fees(con, mca/"mca_fees.csv")  # same schema keys
        if (mca/"mca_eligibility (1).csv").exists(): _load_eligibility_generic(con, mca/"mca_eligibility (1).csv", "MCA")
        if (mca/"mac_documents.csv").exists(): _load_documents_generic(con, mca/"mac_documents.csv", "MCA")

    # MSc
    msc = PROCESSED / "MSC"
    if msc.exists():
        if (msc/"msc_programs (1).csv").exists(): _load_programs_generic(con, msc/"msc_programs (1).csv", "MSc")
        if (msc/"msc_fees (1).csv").exists(): _load_ug_fees(con, msc/"msc_fees (1).csv")
        if (msc/"msc_eligibility (1).csv").exists(): _load_eligibility_generic(con, msc/"msc_eligibility (1).csv", "MSc")
        if (msc/"msc_documents.csv").exists(): _load_documents_generic(con, msc/"msc_documents.csv", "MSc")

# -------------------- MAIN --------------------
def main():
    con = _mk_conn()
    _schema(con)                       # 1) create ALL tables (hostel + academics)
    load_staging_csvs_hostel(con)      # 2) hostel fee CSVs → hostel_fees
    load_hostel_info(con)              # 3) mh_blocks / lh_blocks / hostel_contacts

    # 4) academics from curated CSVs
    _clear_academics(con)
    load_academics_from_processed(con)

    con.commit()
    con.close()
    print(f"[DONE] SQLite DB ready at: {DB_PATH}")

if __name__ == "__main__":
    main()
ABOVE IS LOAD SQLITE

# Stage HOSTEL PDFs → CSVs (Data/staging) for eyeballing before SQLite load.
import pathlib, csv

IN  = pathlib.Path("Data/Raw/HOSTEL")
OUT = pathlib.Path("Data/staging"); OUT.mkdir(parents=True, exist_ok=True)

PDFS = [
  "MH-Senior-FEE-structure-Indian-NRI-FOREIGN-Category-2025-26.pdf",
  "MH-First-Year-FEE-structure-Indian-NRI-FOREIGN-Category-2025-26.pdf",
  "LH-FEE-structure-Indian-Category-2025-26.pdf",
  "LH-FEE-structure-NRI-Foreign-Category-2025-26.pdf",
  "Hostel_info.pdf",
  "Hostel-joint-Affidavit-2025.pdf",
]

def try_camelot(pdf_path: pathlib.Path):
    try:
        import camelot
    except Exception:
        return []
    tables = []
    try:
        t = camelot.read_pdf(str(pdf_path), pages='all', flavor='lattice')
        for i, table in enumerate(t):
            tables.append(("camelot", i, table.df))
    except Exception:
        pass
    return tables

def try_pdfplumber(pdf_path: pathlib.Path):
    import pdfplumber
    tables = []
    try:
        with pdfplumber.open(str(pdf_path)) as pdf:
            for pi, page in enumerate(pdf.pages):
                try:
                    for ti, rows in enumerate(page.extract_tables() or []):
                        tables.append(("pdfplumber", f"{pi}_{ti}", rows))
                except Exception:
                    pass
    except Exception:
        pass
    return tables

def normalize_df(df):
    rows = [list(df.columns)]
    for _, r in df.iterrows(): rows.append([str(x) for x in r.tolist()])
    return rows

def write_csv(rows, out_csv: pathlib.Path):
    with out_csv.open("w", newline="", encoding="utf-8") as f:
        w = csv.writer(f); [w.writerow([c.strip() if isinstance(c, str) else c for c in r]) for r in rows]

def main():
    for pdf_name in PDFS:
        pdf_path = IN / pdf_name
        if not pdf_path.exists():
            print(f"[WARN] Missing {pdf_path}"); continue

        staged = False
        for src, tag, obj in (try_camelot(pdf_path) or []):
            out_csv = OUT / f"{pdf_path.stem}__{src}_{tag}.csv"
            write_csv(normalize_df(obj), out_csv); print(f"[STAGED] {out_csv}"); staged = True

        if not staged:
            for src, tag, rows in (try_pdfplumber(pdf_path) or []):
                out_csv = OUT / f"{pdf_path.stem}__{src}_{tag}.csv"
                write_csv(rows, out_csv); print(f"[STAGED] {out_csv}"); staged = True

        if not staged: print(f"[FAIL] No tables detected in {pdf_path}")

if __name__ == "__main__":
    main()
ABOVE IS STAGE_HOSTEL_TABLES

# app/handlers/academics.py — SQL-first answers for programs/eligibility/documents/academic_fees/scholarships

import re, sqlite3
from typing import Dict, Any, List, Optional, Tuple
from app.utils.aliases import norm, level_alias, category_alias
from app.utils.render  import md_table, bullets, join_sources

ACA_COLUMNS_PROGRAMS = ["Program", "School", "Duration", "Campus", "Source"]
ACA_COLUMNS_ELIG     = ["Level/Program", "Criteria", "Source"]
ACA_COLUMNS_DOCS     = ["Level/Program", "Item", "Details", "Source"]
ACA_COLUMNS_FEES     = ["Level/Program", "AY", "Category", "Tuition", "One-time", "Caution", "Total", "Curr", "Source"]
ACA_COLUMNS_SCH      = ["Level", "Name", "Criteria", "Amount", "Curr", "Source"]

def is_academic_like(q: str) -> bool:
    ql = q.lower()
    keys = [
        "ug","undergrad","pg","mca","msc","programme","program","course",
        "eligibility","criteria","documents","doc to submit","submit docs","admission",
        "academic fee","tuition","semester fee","scholarship","stipend"
    ]
    return any(k in ql for k in keys)

def detect_academics_intent(q: str) -> Dict[str, Any]:
    ql = q.lower()
    level = None
    if any(w in ql for w in ["ug","undergrad","b.tech","btech","bsc","bca","bba"]):
        level = "UG"
    elif any(w in ql for w in ["pg","m.tech","mtech","msc","m.sc","mca","mba"]):
        level = "PG"
    # Program name extraction is open-ended; rely on SQL LIKE later
    want_programs   = any(w in ql for w in ["program","programme","courses","offer","available"])
    want_elig       = "eligib" in ql or "criteria" in ql
    want_docs       = "document" in ql or "submit" in ql or "submission" in ql
    want_fees       = "fee" in ql or "tuition" in ql
    want_schol      = "scholar" in ql
    ay = None
    m = re.search(r"\b(20\d{2})\s*[-/–]\s*(\d{2})\b", ql)
    if m:
        ay = f"{m.group(1)}-{m.group(2)}"
    cat = None
    if "nri" in ql: cat = "NRI"
    elif "foreign" in ql: cat = "Foreign"
    elif "indian" in ql: cat = "Indian"
    return {
        "level": level,
        "program_like": None,  # set later if we capture any quoted string
        "want_programs": want_programs,
        "want_elig": want_elig,
        "want_docs": want_docs,
        "want_fees": want_fees,
        "want_schol": want_schol,
        "ay": ay,
        "category": cat
    }

def _maybe_extract_program_like(q: str) -> Optional[str]:
    # Try picking quoted fragments as program-like hints
    m = re.search(r"['\"]([^\"']{2,80})['\"]", q)
    if m:
        return m.group(1).strip()
    # small heuristic: words after 'for ' or 'in '
    m = re.search(r"\b(for|in)\s+([A-Za-z0-9 &/\-]{2,80})$", q, re.I)
    if m:
        return m.group(2).strip()
    return None

def _sql_like_prog(cond: List[str], args: List[Any], prog_like: Optional[str]):
    if prog_like:
        cond.append("(LOWER(program) LIKE ?)")
        args.append(f"%{prog_like.lower()}%")

def _src(row):
    return row["source_file"] or ""

def answer_academics_sql(con: sqlite3.Connection, q: str) -> Optional[str]:
    if not con: return None
    intent = detect_academics_intent(q)
    if not any([intent["want_programs"], intent["want_elig"], intent["want_docs"], intent["want_fees"], intent["want_schol"]]):
        return None

    level = level_alias(intent["level"]) if intent["level"] else None
    prog_like = _maybe_extract_program_like(q)

    # PROGRAMS
    if intent["want_programs"]:
        sql = "SELECT level, program, school, duration, campus, source_file FROM programs WHERE 1=1"
        cond, args = [], []
        if level:
            cond.append("LOWER(level)=LOWER(?)"); args.append(level)
        _sql_like_prog(cond, args, prog_like)
        rows = con.execute(sql + (" AND " + " AND ".join(cond) if cond else "") + " ORDER BY program", args).fetchall()
        if rows:
            tbl = {
                "columns": ACA_COLUMNS_PROGRAMS,
                "rows": [[r["program"] or "", r["school"] or "", r["duration"] or "", r["campus"] or "", _src(r)] for r in rows]
            }
            return md_table(f"Programs Offered ({level or 'All'})", tbl["columns"], tbl["rows"])
        # fall-through to other branches if nothing

    # ELIGIBILITY
    if intent["want_elig"]:
        sql = "SELECT level, program, criteria, source_file FROM eligibility WHERE 1=1"
        cond, args = [], []
        if level: cond.append("LOWER(level)=LOWER(?)"); args.append(level)
        _sql_like_prog(cond, args, prog_like)
        rows = con.execute(sql + (" AND " + " AND ".join(cond) if cond else ""), args).fetchall()
        if rows:
            tbl = {
                "columns": ACA_COLUMNS_ELIG,
                "rows": [[(r["level"] or "") + (" / " + (r["program"] or "")), r["criteria"] or "", _src(r)] for r in rows]
            }
            return md_table("Eligibility", tbl["columns"], tbl["rows"])

    # DOCUMENTS
    if intent["want_docs"]:
        sql = "SELECT level, program, item, details, source_file FROM documents_required WHERE 1=1"
        cond, args = [], []
        if level: cond.append("LOWER(level)=LOWER(?)"); args.append(level)
        _sql_like_prog(cond, args, prog_like)
        rows = con.execute(sql + (" AND " + " AND ".join(cond) if cond else "") + " ORDER BY level, program, item", args).fetchall()
        if rows:
            tbl = {"columns": ACA_COLUMNS_DOCS,
                   "rows": [[(r["level"] or "") + (" / " + (r["program"] or "")), r["item"] or "", r["details"] or "", _src(r)]
                            for r in rows]}
            return md_table("Documents to Submit", tbl["columns"], tbl["rows"])

    # ACADEMIC FEES
    if intent["want_fees"]:
        sql = """SELECT level, program, ay, category, tuition, one_time, caution, total, currency, source_file
                 FROM academic_fees WHERE 1=1"""
        cond, args = [], []
        if level: cond.append("LOWER(level)=LOWER(?)"); args.append(level)
        if intent["ay"]: cond.append("ay=?"); args.append(intent["ay"])
        if intent["category"]:
            cond.append("LOWER(category)=LOWER(?)"); args.append(category_alias(intent["category"]))
        _sql_like_prog(cond, args, prog_like)
        order = " ORDER BY level, program, ay, category"
        rows = con.execute(sql + (" AND " + " AND ".join(cond) if cond else "") + order, args).fetchall()
        if rows:
            tbl = {"columns": ACA_COLUMNS_FEES,
                   "rows": [[(r["level"] or "") + (" / " + (r["program"] or "")), r["ay"] or "", r["category"] or "",
                             r["tuition"] or "", r["one_time"] or "", r["caution"] or "", r["total"] or "",
                             r["currency"] or "", _src(r)] for r in rows]}
            return md_table("Academic Fee Details", tbl["columns"], tbl["rows"])

    # SCHOLARSHIPS
    if intent["want_schol"]:
        sql = "SELECT level, name, criteria, amount, currency, source_file FROM scholarships WHERE 1=1"
        cond, args = [], []
        if level: cond.append("LOWER(level)=LOWER(?)"); args.append(level)
        rows = con.execute(sql + (" AND " + " AND ".join(cond) if cond else "") + " ORDER BY level, name", args).fetchall()
        if rows:
            tbl = {"columns": ACA_COLUMNS_SCH,
                   "rows": [[r["level"] or "", r["name"] or "", r["criteria"] or "", r["amount"] or "", r["currency"] or "", _src(r)]
                            for r in rows]}
            return md_table("Scholarships", tbl["columns"], tbl["rows"])

    return None
ABOVE ONE IS ACADEMICS app/handlers - section 

# app/handlers/hostels.py — hostel routing (contacts/blocks/landlines/fees) with your existing vibe

import re, sqlite3
from typing import Dict, Any, List, Optional, Tuple
from app.utils.aliases import norm
from app.utils.render  import md_table, bullets

def detect_hostel_intent(q: str) -> Dict[str, Any]:
    qq = q.lower()
    hostelish = any(w in qq for w in [
        "hostel","block","mh","lh","boys","girls","mens","ladies",
        "warden","supervisor","director","manager","landline","mess","laundry","fee","fees"
    ])
    g = None
    if any(w in qq for w in ["men", "boys", "mh", "mens"]): g = "MH"
    if any(w in qq for w in ["ladies", "girls", "women", "lh"]): g = "LH" if g is None else g

    wants_landlines = hostelish and any(w in qq for w in ["landline", "phone", "contact number", "call"])
    role_hit = any(w in qq for w in ["director","chief warden","associate chief","warden","manager","assistant manager","supervisor"])
    wants_contacts  = hostelish and role_hit or role_hit
    wants_blocks    = hostelish and "block" in qq and any(w in qq for w in ["name","names","code","codes","list","all"])
    wants_fees      = hostelish and ("fee" in qq or "mess" in qq or "room" in qq)

    specific_block  = None
    m = re.search(r"\b(MH|LH)\s*-?\s*([A-Z]{1,2})(?:\s*ANNEX)?\b", q, re.I)
    if m:
        specific_block = (m.group(1).upper() + " " + m.group(2).upper()).strip()
        if "annex" in qq: specific_block += " ANNEX"

    reverse_phone = None
    m = re.search(r"(\+?\d[\d\s\-]{7,})", q)
    if m: reverse_phone = m.group(1)
    reverse_mail = None
    m = re.search(r"([A-Za-z0-9._%+\-]+@[A-Za-z0-9.\-]+\.[A-Za-z]{2,})", q)
    if m: reverse_mail = m.group(1)

    return {
        "hostelish": hostelish, "gender": g,
        "landlines": wants_landlines, "contacts": wants_contacts,
        "blocks": wants_blocks, "fees": wants_fees,
        "specific_block": specific_block,
        "reverse_phone": reverse_phone, "reverse_mail": reverse_mail
    }

def reverse_lookup_bundle(con: sqlite3.Connection, intent: Dict[str,Any]) -> str:
    # reuse the mh_blocks/lh_blocks + hostel_contacts for lookups
    q_phone = intent["reverse_phone"]; q_mail = intent["reverse_mail"]

    def _get_all_contacts():
        try:
            return con.execute("SELECT role,name,phone,email FROM hostel_contacts").fetchall()
        except Exception:
            return []

    def _get_mh_blocks():
        try:
            return con.execute("SELECT block_code, block_name, landline, IFNULL(email,'') as email FROM mh_blocks").fetchall()
        except Exception:
            return []

    def _get_lh_blocks():
        try:
            return con.execute("SELECT block_code, block_code AS block_name, landline, '' as email FROM lh_blocks").fetchall()
        except Exception:
            return []

    hits = []
    if q_phone:
        n = re.sub(r"\D", "", q_phone)
        for r in _get_all_contacts():
            ph = re.sub(r"\D", "", r["phone"] or "")
            if n and ph.find(n) != -1:
                hits.append(["Contact","-", r["role"] or "", r["name"] or "", r["phone"] or "", r["email"] or ""])
        for r in _get_mh_blocks():
            ph = re.sub(r"\D", "", r["landline"] or "")
            if n and ph.find(n) != -1:
                hits.append(["Block","MH", r["block_code"] or "", r["block_name"] or "", r["landline"] or "", r["email"] or ""])
        for r in _get_lh_blocks():
            ph = re.sub(r"\D", "", r["landline"] or "")
            if n and ph.find(n) != -1:
                hits.append(["Block","LH", r["block_code"] or "", r["block_name"] or "", r["landline"] or "", ""])
        if hits:
            return md_table("Reverse lookup — phone", ["Type","Hostel","Role/Code","Name","Phone","Email"], hits)
        return "_No matches for that phone number._"

    if q_mail:
        mail = q_mail.strip().lower()
        for r in _get_all_contacts():
            if (r["email"] or "").lower() == mail:
                hits.append(["Contact","-", r["role"] or "", r["name"] or "", r["phone"] or "", r["email"] or ""])
        for r in _get_mh_blocks():
            if (r["email"] or "").lower() == mail:
                hits.append(["Block","MH", r["block_code"] or "", r["block_name"] or "", r["landline"] or "", r["email"] or ""])
        if hits:
            return md_table("Reverse lookup — email", ["Type","Hostel","Role/Code","Name","Phone","Email"], hits)
        return "_No matches for that email._"

    return "_No reverse lookup target found._"

def answer_hostel_sqlfirst(con: sqlite3.Connection, q: str, intent: Dict[str,Any]) -> Optional[str]:
    if not con: return None
    ql = q.lower()

    # Landlines / blocks lists from mh_blocks/lh_blocks
    if intent["landlines"] or intent["blocks"] or intent["specific_block"]:
        if intent["specific_block"]:
            code = intent["specific_block"]
            rows = []
            mhr = con.execute("SELECT 'MH' as hostel, block_code, block_name, landline, IFNULL(email,'') as email FROM mh_blocks WHERE UPPER(block_code)=UPPER(?)", (code,)).fetchall()
            lhr = con.execute("SELECT 'LH' as hostel, block_code, block_code as block_name, landline, '' as email FROM lh_blocks WHERE UPPER(block_code)=UPPER(?)", (code,)).fetchall()
            for r in list(mhr) + list(lhr):
                rows.append([r["hostel"], r["block_code"], r["block_name"], r["landline"], r["email"]])
            return md_table("Block Details", ["Hostel","Block Code","Block Name","Landline","Email"], rows) if rows else "_No matching block found._"

        # list or landlines
        if intent["gender"] in (None, "MH"):
            mh = con.execute("SELECT block_code, block_name, landline, IFNULL(email,'') as email FROM mh_blocks").fetchall()
            if mh:
                out = md_table("Men’s Hostel — Block-wise Landlines", ["Block Code","Block Name","Landline","Email"],
                               [[r["block_code"], r["block_name"], r["landline"], r["email"]] for r in mh])
            else:
                out = ""
        else:
            out = ""
        if intent["gender"] in (None, "LH"):
            lh = con.execute("SELECT block_code, block_code as block_name, landline, '' as email FROM lh_blocks").fetchall()
            if lh:
                t = md_table("Ladies’ Hostel — Block-wise Landlines", ["Block Code","Block Name","Landline","Email"],
                             [[r["block_code"], r["block_name"], r["landline"], r["email"]] for r in lh])
                out = (out + "\n\n" + t) if out else t
        return out or "_No hostel blocks/landlines found in DB._"

    # Contacts (hostel_contacts)
    if intent["contacts"]:
        rows = con.execute("SELECT role, name, phone, email FROM hostel_contacts ORDER BY role").fetchall()
        if not rows: return "_No contacts available._"
        lines = []
        for r in rows:
            who = f"{r['role']}: {r['name']}" if r["name"] else r["role"]
            bits = []
            if r["phone"]: bits.append(r["phone"])
            if r["email"]: bits.append(r["email"])
            lines.append(f"{who} — " + " | ".join(bits) if bits else who)
        return bullets(lines, "Hostel — Key Contacts")

    # Fees: readable table from hostel_fees + blocks
    if intent["fees"]:
        cond, args = [], []
        if "2025-26" in ql:
            cond.append("hf.ay=?"); args.append("2025-26")
        if "nri" in ql: cond.append("LOWER(hf.category)='nri'")
        if "indian" in ql: cond.append("LOWER(hf.category)='indian'")
        if "foreign" in ql: cond.append("LOWER(hf.category)='foreign'")
        sql = f"""
        SELECT b.display_name as block, b.gender, b.level, hf.ay, hf.category, hf.occupancy, hf.ac, hf.mess_type,
               hf.room_mess_fee, hf.admission_fee, hf.caution_deposit, hf.other_fee, hf.total_fee, hf.currency, hf.source_file
        FROM hostel_fees hf
        JOIN blocks b ON b.id=hf.block_id
        {"WHERE " + " AND ".join(cond) if cond else ""}
        ORDER BY b.gender, b.level, b.block_name, hf.occupancy, hf.ac DESC, hf.mess_type
        """
        rows = con.execute(sql, args).fetchall()
        if rows:
            return md_table("Hostel Fee Details", 
                ["Block","Gender","Level","AY","Category","Occ","AC","Mess","Room+Mess","Admission","Caution","Other","Total","Curr","Source"],
                [[r["block"], r["gender"] or "", r["level"] or "", r["ay"] or "", r["category"] or "", r["occupancy"] or "",
                  "AC" if (r["ac"]==1) else ("Non-AC" if (r["ac"]==0) else ""), r["mess_type"] or "", r["room_mess_fee"] or "",
                  r["admission_fee"] or "", r["caution_deposit"] or "", r["other_fee"] or "", r["total_fee"] or "", r["currency"] or "", r["source_file"] or ""]
                 for r in rows
                ])
        return "_No hostel fee rows matched your filters._"

    return None
ABOVE IS HOSTELS app/handlers

# app/handlers/rules.py — refund/attendance/discipline/exam etc.

import re, sqlite3
from typing import Optional
from app.utils.render import md_table, bullets

RULE_KEYS = ["refund","attendance","discipline","anti-ragging","anti ragging","exam","examination"]

def detect_rules_intent(q: str) -> bool:
    ql = q.lower()
    return any(k in ql for k in RULE_KEYS)

def answer_rules_sql(con: sqlite3.Connection, q: str) -> Optional[str]:
    if not con: return None
    ql = q.lower()
    cat = None
    for k in RULE_KEYS:
        if k in ql:
            cat = "anti-ragging" if k in ("anti ragging","anti-ragging") else k
            break
    sql = "SELECT category, title, text, IFNULL(ay,'') as ay, source_file FROM rules"
    args = []
    if cat:
        sql += " WHERE LOWER(category)=LOWER(?)"
        args.append(cat)
    sql += " ORDER BY ay DESC, title"
    rows = con.execute(sql, args).fetchall()
    if not rows:
        return None
    lines = [f"**{r['title']}** — {r['text']} *(AY: {r['ay']} | src: {r['source_file']})*" for r in rows]
    return bullets(lines, f"Rules — {cat.title() if cat else 'All'}")
ABOVE IS RULES app/handlers

import re

def norm(s: str) -> str:
    return re.sub(r"[^a-z0-9]+","", (s or "").lower())


def level_alias(txt: str) -> str:
    if not txt: return ""
    t = txt.lower()
    if t in ("ug","undergrad","undergraduate","btech","b.tech","bsc","bca","bba"): return "UG"
    if t in ("pg","postgrad","postgraduate","mtech","m.tech","msc","m.sc","mca","mba"): return "PG"
    return txt

def category_alias(txt: str) -> str:
    if not txt: return ""
    t = txt.lower()
    if "nri" in t: return "NRI"
    if "foreign" in t: return "Foreign"
    if "indian" in t: return "Indian"
    return txt
ABOVE IS ALIASES app/utils

# app/utils/fallback_rag.py — FAISS retrieval + Gemini embeddings + simple summaries

import os, sys, re
from pathlib import Path
from typing import List, Tuple
from langchain_community.vectorstores import FAISS
from langchain_core.embeddings import Embeddings as LCEmbeddings

class GeminiLCEmbeddings(LCEmbeddings):
    def __init__(self, model: str = "models/text-embedding-004", api_key_env: str = "GEMINI_API_KEY"):
        import google.generativeai as genai
        api_key = os.getenv(api_key_env)
        if not api_key:
            print("[ERROR] GEMINI_API_KEY not set.")
            sys.exit(1)
        genai.configure(api_key=api_key)
        self.genai = genai
        self.model = model
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return [self.genai.embed_content(model=self.model, content=t)["embedding"] for t in texts]
    def embed_query(self, text: str) -> List[float]:
        return self.genai.embed_content(model=self.model, content=text)["embedding"]

def _keyword_score(text: str, q: str) -> int:
    qwords = [w for w in re.findall(r"[a-z0-9]+", q.lower()) if len(w) > 2]
    t = text.lower()
    return sum(t.count(w) for w in qwords)

def _postfilter_and_rerank(docs, query: str, topn: int = 12):
    scored = []
    for d in docs:
        s = _keyword_score(d.page_content or "", query)
        scored.append((s, d))
    scored.sort(key=lambda x: x[0], reverse=True)
    return [d for s, d in scored][:topn]

def _summarize(text: str) -> str:
    blob = re.sub(r"\s+", " ", text or "").strip()
    if not blob: return ""
    parts = re.split(r"(?<=[.!?])\s+", blob)
    return " ".join(parts[:8])

def faiss_answer_or_summary(index_dir: Path, collection: str, q: str) -> str:
    emb = GeminiLCEmbeddings()
    store = FAISS.load_local(str(index_dir), embeddings=emb, index_name=collection, allow_dangerous_deserialization=True)
    docs = store.similarity_search(q, k=40)
    docs = _postfilter_and_rerank(docs, q, topn=12)
    if not docs:
        return "_I couldn’t find enough context to answer that from your PDFs._"
    ctx, sources = [], []
    for d in docs:
        md = d.metadata or {}
        src = md.get("source_file") or md.get("source_title") or "unknown"
        sources.append(src); ctx.append(d.page_content or "")
    text = "\n\n".join(ctx)
    para = _summarize(text)
    srcs = []
    seen = set()
    for s in sources:
        if s and s not in seen:
            seen.add(s); srcs.append(s)
            if len(srcs) >= 5: break
    return f"**Answer (from your PDFs):** {para}\n\n*Sources:* {', '.join(srcs)}"
ABOVE IS FALLBACK.PY app/utils

from typing import List, Optional

def md_table(title: str, cols: List[str], rows: List[List[str]]) -> str:
    seen=set(); uniq=[]
    for r in rows:
        tup=tuple("" if c is None else str(c) for c in r)
        if tup in seen: continue
        seen.add(tup); uniq.append(list(tup))
    out = []
    if title: out.append(f"**{title}**")
    out.append("| " + " | ".join(cols) + " |")
    out.append("|" + "|".join(["---"]*len(cols)) + "|")
    for r in uniq:
        out.append("| " + " | ".join("" if c is None else str(c) for c in r) + " |")
    return "\n".join(out)

def bullets(lines: List[str], title: Optional[str]=None) -> str:
    if not lines: return ""
    seen=set(); out_lines=[]
    for ln in lines:
        if ln in seen: continue
        seen.add(ln); out_lines.append(ln)
    out = [f"**{title}**" if title else ""]
    out += [f"- {ln}" for ln in out_lines]
    return "\n".join([x for x in out if x])

def join_sources(srcs: List[str], maxn: int = 5) -> str:
    if not srcs: return ""
    uniq = []
    seen = set()
    for s in srcs:
        if not s or s in seen: continue
        seen.add(s); uniq.append(s)
        if len(uniq) >= maxn: break
    return ", ".join(uniq)
ABOVE IS RENDER.PY app/utils

# fix_router_and_test.py  — put this in project root and run with python
import os, pathlib, sqlite3, textwrap, sys

ROOT = pathlib.Path(__file__).resolve().parent
APP  = ROOT / "app"
HAND = APP / "handlers"
UTIL = APP / "utils"
DB   = ROOT / "Data" / "sql" / "vit_vellore.db"

for p in [APP, HAND, UTIL]:
    p.mkdir(parents=True, exist_ok=True)
    (p / "__init__.py").write_text("", encoding="utf-8")

sql_router_code = r"""
import sqlite3, pathlib, re
from typing import Dict, Any, List

DB = pathlib.Path('Data/sql/vit_vellore.db')

ACA_KEYWORDS = {
    'programs': ['program','course','degree','b.tech','m.tech','mca','msc','programme'],
    'eligibility': ['eligibility','qualify','criteria','requirements','min marks'],
    'documents': ['document','documents','docs','upload','certificate','proof','bonafide'],
    'fees': ['tuition','fee','fees','academic fee','semester fee'],
    'scholarships': ['scholarship','waiver','merit','financial aid']
}

def detect_structured_intent(q: str) -> str:
    ql = q.lower()
    if any(k in ql for k in ['contact','phone','email','supervisor','warden','director','manager']):
        return 'contacts'
    if 'block' in ql and any(k in ql for k in ['name','names','list','all','codes','code']):
        return 'blocks'
    if any(k in ql for k in ['hostel','room','mess','laundry','mh','lh']):
        return 'hostel'
    for tag, words in ACA_KEYWORDS.items():
        if any(w in ql for w in words):
            return tag
    if any(k in ql for k in ['fee','fees','tuition']):
        return 'fees'
    return 'text'

def parse_filters(q: str) -> Dict[str, Any]:
    ql = q.lower()
    f = {'ay': None, 'gender': None, 'category': None, 'level': None, 'program_like': None, 'level_like': None}
    m = re.search(r'\\b(20\\d{2})\\b', ql)
    if m:
        yr = m.group(1); f['ay'] = f'{yr}-{str(int(yr[-2:])+1).zfill(2)}'
    if any(k in ql for k in ['boy','boys','men','mh','mens']): f['gender'] = 'Male'
    if any(k in ql for k in ['girl','girls','ladies','lh','women']): f['gender'] = 'Female'
    if 'nri' in ql: f['category'] = 'NRI'
    elif 'foreign' in ql: f['category'] = 'Foreign'
    elif 'indian' in ql: f['category'] = 'Indian'
    if 'senior' in ql: f['level'] = 'Senior'
    if 'first year' in ql or 'fresh' in ql: f['level'] = 'First-Year'
    m = re.search(r'\\b(b\\.?tech|m\\.?tech|mca|msc|cse|ece|mechanical|biotech|ai|data)\\b', ql)
    if m: f['program_like'] = m.group(1).replace('.','')
    if 'ug' in ql: f['level_like'] = 'UG'
    if 'pg' in ql: f['level_like'] = 'PG'
    return f

def _mk():
    con = sqlite3.connect(DB)
    con.row_factory = sqlite3.Row
    return con

def _safe_fetch(con, sql, args) -> List[sqlite3.Row]:
    try: return con.execute(sql, args).fetchall()
    except sqlite3.OperationalError: return []

# -------- HOSTEL --------
def sql_hostel_overview(f: Dict[str,Any], limit_rows: int = 800) -> Dict[str, Any]:
    con = _mk()
    where = ['1=1']; args=[]
    if f['gender']:   where.append('IFNULL(b.gender,\"\")=IFNULL(?, \"\")');   args.append(f['gender'])
    if f['level']:    where.append('IFNULL(b.level,\"\")=IFNULL(?, \"\")');    args.append(f['level'])
    if f['ay']:       where.append('IFNULL(hf.ay,\"\")=IFNULL(?, \"\")');      args.append(f['ay'])
    if f['category']: where.append('IFNULL(hf.category,\"\")=IFNULL(?, \"\")');args.append(f['category'])
    where.append(\"\"\"(
        COALESCE(hf.total_fee,'')<>'' OR COALESCE(hf.room_mess_fee,'')<>'' OR
        COALESCE(hf.admission_fee,'')<>'' OR COALESCE(hf.caution_deposit,'')<>'' OR
        COALESCE(hf.occupancy,'')<>'' OR COALESCE(hf.mess_type,'')<>''
    )\"\"\")
    sql = f\"\"\"
    SELECT b.display_name AS block, b.gender, b.level, b.block_type,
           hf.ay, hf.category, hf.occupancy, hf.ac, hf.mess_type,
           hf.room_mess_fee, hf.admission_fee, hf.caution_deposit, hf.other_fee, hf.total_fee, hf.currency,
           hf.source_file
    FROM hostel_fees hf
    JOIN blocks b ON b.id = hf.block_id
    WHERE {' AND '.join(where)}
    ORDER BY b.block_type, block, hf.occupancy, hf.ac DESC, hf.mess_type
    \"\"\"
    rows = _safe_fetch(con, sql, args); con.close()
    columns = ['Block','Gender','Level','Type','AY','Category','Occ','AC','Mess',
               'Room+Mess','Admission','Caution','Other','Total','Curr','Source']
    tbl = {'title':'Hostel Fee Details (Vellore)', 'columns':columns, 'rows':[]}
    for r in rows[:limit_rows]:
        tbl['rows'].append([
            r['block'] or '', r['gender'] or '', r['level'] or '', r['block_type'] or '',
            r['ay'] or '', r['category'] or '', r['occupancy'] or '',
            'AC' if (r['ac']==1) else ('Non-AC' if (r['ac']==0) else ''),
            r['mess_type'] or '', r['room_mess_fee'] or '', r['admission_fee'] or '',
            r['caution_deposit'] or '', r['other_fee'] or '', r['total_fee'] or '',
            r['currency'] or '', r['source_file'] or ''
        ])
    return {'table': tbl, 'bullets': []}

def sql_block_contacts(_f: Dict[str,Any]) -> Dict[str,Any]:
    con = _mk()
    rows = _safe_fetch(con, \"
        SELECT '' AS block, name, role, phone, email
        FROM hostel_contacts
        ORDER BY role, name
    \", [])
    con.close()
    cols = ['Block','Name','Role','Phone','Email']
    tbl = {'title':'Hostel Contacts', 'columns':cols, 'rows':[
        [r['block'], r['name'] or '', r['role'] or '', r['phone'] or '', r['email'] or '']
        for r in rows
    ]}
    return {'table': tbl, 'bullets': []}

def sql_list_blocks(f: Dict[str,Any]) -> Dict[str,Any]:
    con = _mk()
    if f['gender'] == 'Female':
        sql = \"SELECT block_code AS display_name, 'Female' AS gender FROM lh_blocks ORDER BY block_code\"
    elif f['gender'] == 'Male':
        sql = \"SELECT COALESCE(block_name, block_code) AS display_name, 'Male' AS gender FROM mh_blocks ORDER BY block_code\"
    else:
        sql = \"
        SELECT COALESCE(block_name, block_code) AS display_name, 'Male' AS gender FROM mh_blocks
        UNION ALL
        SELECT block_code AS display_name, 'Female' AS gender FROM lh_blocks
        ORDER BY gender, display_name
        \"
    rows = _safe_fetch(con, sql, []); con.close()
    cols = ['Block','Gender']
    tbl = {'title':'Hostel Blocks', 'columns':cols, 'rows':[
        [r['display_name'] or '', r['gender'] or ''] for r in rows
    ]}
    return {'table': tbl, 'bullets': []}

# -------- ACADEMICS --------
def _like_clause(col: str, val: str, wh: list, args: list):
    if val:
        wh.append(f'LOWER({col}) LIKE ?')
        args.append(f'%{val.lower()}%')

def sql_programs(f: Dict[str,Any], q: str) -> Dict[str,Any]:
    con = _mk()
    where, args = ['1=1'], []
    _like_clause('level', f.get('level_like'), where, args)
    _like_clause('program', f.get('program_like'), where, args)
    _like_clause('program', q, where, args)
    rows = _safe_fetch(con, f\"
        SELECT level, program, school, duration, campus, source_file
        FROM programs
        WHERE {' AND '.join(where)}
        ORDER BY level, program
    \", args); con.close()
    cols = ['Level','Program','School','Duration','Campus','Source']
    return {'table':{'title':'Programs','columns':cols,'rows':[
        [r['level'] or '', r['program'] or '', r['school'] or '', r['duration'] or '', r['campus'] or '', r['source_file'] or '']
        for r in rows
    ]}, 'bullets':[]}

def sql_eligibility(f: Dict[str,Any], q: str) -> Dict[str,Any]:
    con = _mk()
    where, args = ['1=1'], []
    _like_clause('level', f.get('level_like'), where, args)
    _like_clause('program', f.get('program_like'), where, args)
    _like_clause('criteria', q, where, args)
    rows = _safe_fetch(con, f\"
        SELECT level, program, criteria, source_file
        FROM eligibility
        WHERE {' AND '.join(where)}
        ORDER BY level, program
    \", args); con.close()
    cols = ['Level','Program','Criteria','Source']
    return {'table':{'title':'Eligibility','columns':cols,'rows':[
        [r['level'] or '', r['program'] or '', r['criteria'] or '', r['source_file'] or '']
        for r in rows
    ]}, 'bullets':[]}

def sql_documents(f: Dict[str,Any], q: str) -> Dict[str,Any]:
    con = _mk()
    where, args = ['1=1'], []
    _like_clause('level', f.get('level_like'), where, args)
    _like_clause('program', f.get('program_like'), where, args)
    _like_clause('item', q, where, args)
    rows = _safe_fetch(con, f\"
        SELECT level, program, item, details, source_file
        FROM documents_required
        WHERE {' AND '.join(where)}
        ORDER BY level, program, item
    \", args); con.close()
    cols = ['Level','Program','Document','Details','Source']
    return {'table':{'title':'Documents Required','columns':cols,'rows':[
        [r['level'] or '', r['program'] or '', r['item'] or '', r['details'] or '', r['source_file'] or '']
        for r in rows
    ]}, 'bullets':[]}

def sql_academic_fees(f: Dict[str,Any], q: str) -> Dict[str,Any]:
    con = _mk()
    where, args = ['1=1'], []
    _like_clause('program', f.get('program_like'), where, args)
    if f.get('level_like'): where.append('LOWER(level)=?'); args.append(f['level_like'].lower())
    if f.get('category'):  where.append('IFNULL(category,\"\")=IFNULL(?, \"\")'); args.append(f['category'])
    if f.get('ay'):        where.append('IFNULL(ay,\"\")=IFNULL(?, \"\")');        args.append(f['ay'])
    _like_clause('program', q, where, args)
    rows = _safe_fetch(con, f\"
        SELECT level, program, category, ay, tuition, one_time, caution, total, currency, source_file
        FROM academic_fees
        WHERE {' AND '.join(where)}
        ORDER BY level, program, category, ay
    \", args); con.close()
    cols = ['Level','Program','Category','AY','Tuition','One-time','Caution','Total','Curr','Source']
    return {'table':{'title':'Academic Fees','columns':cols,'rows':[
        [r['level'] or '', r['program'] or '', r['category'] or '', r['ay'] or '',
         r['tuition'] or '', r['one_time'] or '', r['caution'] or '', r['total'] or '',
         r['currency'] or '', r['source_file'] or '']
        for r in rows
    ]}, 'bullets':[]}

def sql_scholarships(f: Dict[str,Any], q: str) -> Dict[str,Any]:
    con = _mk()
    where, args = ['1=1'], []
    _like_clause('level', f.get('level_like'), where, args)
    _like_clause('name', q, where, args)
    rows = _safe_fetch(con, f\"
        SELECT level, name, criteria, amount, currency, source_file
        FROM scholarships
        WHERE {' AND '.join(where)}
        ORDER BY level, name
    \", args); con.close()
    cols = ['Level','Name','Criteria','Amount','Curr','Source']
    return {'table':{'title':'Scholarships','columns':cols,'rows':[
        [r['level'] or '', r['name'] or '', r['criteria'] or '', r['amount'] or '', r['currency'] or '', r['source_file'] or '']
        for r in rows
    ]}, 'bullets':[]}
""".lstrip()

rag_answer_code = r"""
\"\"\"SQL-first router; FAISS fallback.\"\"\"
from app.sql_router import (
    detect_structured_intent, parse_filters,
    sql_hostel_overview, sql_block_contacts, sql_list_blocks,
    sql_programs, sql_eligibility, sql_documents, sql_academic_fees, sql_scholarships
)

def _fmt_table(tbl: dict) -> str:
    if not tbl or not tbl.get('rows'):
        return '_No matching rows._'
    title  = f\"**{tbl.get('title','Results')}**\"
    cols   = tbl['columns']
    header = ' | '.join(cols)
    sep    = ' | '.join(['---']*len(cols))
    lines  = [title, '', header, sep]
    for r in tbl['rows']:
        lines.append(' | '.join(str(x) if x is not None else '' for x in r))
    return '\\n'.join(lines)

def _pack(table_dict=None, bullets=None):
    parts = []
    if table_dict: parts.append(_fmt_table(table_dict))
    if bullets:    parts.append('\\n'.join(f'- {b}' for b in bullets))
    return '\\n\\n'.join(parts) if parts else '_No results._'

def _sql_route(q: str) -> str | None:
    intent = detect_structured_intent(q)
    f = parse_filters(q)
    if intent == 'contacts':
        return _pack(sql_block_contacts(f).get('table'))
    if intent == 'blocks':
        return _pack(sql_list_blocks(f).get('table'))
    if intent in ('hostel','tabular'):
        return _pack(sql_hostel_overview(f).get('table'))
    if intent == 'programs':
        return _pack(sql_programs(f, q).get('table'))
    if intent == 'eligibility':
        return _pack(sql_eligibility(f, q).get('table'))
    if intent == 'documents':
        return _pack(sql_documents(f, q).get('table'))
    if intent == 'fees':
        return _pack(sql_academic_fees(f, q).get('table'))
    if intent == 'scholarships':
        return _pack(sql_scholarships(f, q).get('table'))
    return None

def answer(query: str) -> str:
    sql_text = _sql_route(query)
    if sql_text is not None:
        return sql_text
    try:
        from app.utils.fallback_rag import answer as rag_fallback
        rag = rag_fallback(query, max_chunks=6, max_tokens=450)
        return rag or '_I couldn't find that in my sources._'
    except Exception:
        return '_I couldn't find that in my sources._'

if __name__ == '__main__':
    import sys
    q = ' '.join(sys.argv[1:]) or 'Show MH Senior NRI hostel fees 2025'
    print(answer(q))
""".lstrip()

# Write files
(APP / "sql_router.py").write_text(sql_router_code, encoding="utf-8")
(APP / "rag_answer.py").write_text(rag_answer_code, encoding="utf-8")

print("[OK] Wrote app/sql_router.py and app/rag_answer.py")

# Basic DB sanity
if not DB.exists():
    print(f"[WARN] DB not found at {DB}. Run ETL/load_sqlite.py first.")
else:
    con = sqlite3.connect(DB)
    counts = {}
    for t in ["programs","eligibility","documents_required","academic_fees","scholarships","hostel_fees"]:
        try:
            counts[t] = con.execute(f"select count(*) from {t}").fetchone()[0]
        except Exception as e:
            counts[t] = f"ERR: {e}"
    con.close()
    print("[DB] counts:", counts)

# Quick functional smoke tests
try:
    from app.sql_router import detect_structured_intent, parse_filters, sql_documents, sql_programs, sql_academic_fees
    print("[TEST] intent(documents):", detect_structured_intent("UG documents required"))
    f = parse_filters("UG documents required")
    r = sql_documents(f, "UG documents required")
    print("[TEST] documents rows:", len(r["table"]["rows"]))

    print("[TEST] intent(programs):", detect_structured_intent("UG programs CSE"))
    f2 = parse_filters("UG programs CSE")
    r2 = sql_programs(f2, "UG programs CSE")
    print("[TEST] programs rows:", len(r2["table"]["rows"]))

    print("[TEST] fees intent:", detect_structured_intent("B.Tech tuition fee Indian 2025"))
    f3 = parse_filters("B.Tech tuition fee Indian 2025")
    r3 = sql_academic_fees(f3, "B.Tech tuition fee Indian 2025")
    print("[TEST] fees rows:", len(r3["table"]["rows"]))
except Exception as e:
    print("[TEST] error:", e)

# Print final answers preview
try:
    from app.rag_answer import answer
    for q in [
        "UG documents required",
        "UG programs CSE",
        "B.Tech tuition fee Indian 2025"
    ]:
        print("\n=== Q:", q)
        out = answer(q)
        # show first 25 lines to keep console readable
        lines = out.splitlines()
        print("\n".join(lines[:25] if len(lines)>25 else lines))
except Exception as e:
    print("[ANSWER] error:", e)
ABOVE ONE IS FIC_ROUTER_AND_TEST.PY from app folder

"""
Router: SQL-first for fees/programs/eligibility/documents/scholarships/hostel.
Terse tables + tiny bullets. FAISS fallback only for long-form explainers.
"""

from app.sql_router import (
    detect_structured_intent, parse_filters,
    sql_hostel_overview, sql_block_contacts, sql_list_blocks,
    sql_programs, sql_eligibility, sql_documents, sql_academic_fees, sql_scholarships
)

def _fmt_table(tbl: dict) -> str:
    if not tbl or not tbl.get("rows"):
        return "_No matching rows._"
    title  = f"**{tbl.get('title','Results')}**"
    cols   = tbl["columns"]
    header = " | ".join(cols)
    sep    = " | ".join(["---"]*len(cols))
    lines  = [title, "", header, sep]
    for r in tbl["rows"]:
        lines.append(" | ".join(str(x) if x is not None else "" for x in r))
    return "\n".join(lines)

def _fmt_bullets(bullets: list[str]) -> str:
    return "\n".join(f"- {b}" for b in bullets) if bullets else ""

def _pack(table_dict=None, bullets=None):
    parts = []
    if table_dict: parts.append(_fmt_table(table_dict))
    if bullets:    parts.append(_fmt_bullets(bullets))
    return "\n\n".join(parts) if parts else "_No results._"

def _sql_route(q: str) -> str | None:
    intent = detect_structured_intent(q)
    f = parse_filters(q)

    # HOSTEL
    if intent == "contacts":
        return _pack(sql_block_contacts(f).get("table"))
    if intent == "blocks":
        return _pack(sql_list_blocks(f).get("table"))
    if intent in ("hostel", "tabular"):
        return _pack(sql_hostel_overview(f).get("table"))

    # ACADEMICS
    if intent == "programs":
        return _pack(sql_programs(f, q).get("table"))
    if intent == "eligibility":
        return _pack(sql_eligibility(f, q).get("table"))
    if intent == "documents":
        return _pack(sql_documents(f, q).get("table"))
    if intent == "fees":
        return _pack(sql_academic_fees(f, q).get("table"))
    if intent == "scholarships":
        return _pack(sql_scholarships(f, q).get("table"))

    return None

def answer(query: str) -> str:
    sql_text = _sql_route(query)
    if sql_text is not None:
        return sql_text

    # FAISS fallback (tight + sourcey)
    try:
        from app.utils.fallback_rag import answer as rag_fallback
        rag = rag_fallback(query, max_chunks=6, max_tokens=450)
        return rag or "_I couldn't find that in my sources._"
    except Exception:
        return "_I couldn't find that in my sources._"

if __name__ == "__main__":
    import sys
    q = " ".join(sys.argv[1:]) or "Show MH Senior NRI hostel fees 2025"
    print(answer(q))
ABOVE IS RAG ANSWER.PY  app folder

import sqlite3, pathlib, re
from typing import Dict, Any, List

DB = pathlib.Path("Data/sql/vit_vellore.db")

ACA_KEYWORDS = {
    "programs": ["program","course","degree","b.tech","m.tech","mca","msc"],
    "eligibility": ["eligibility","qualify","criteria","requirements","min marks"],
    "documents": ["document","docs","upload","certificate","proof","bonafide"],
    "fees": ["tuition","fee","fees","academic fee","semester fee"],
    "scholarships": ["scholarship","waiver","merit","financial aid"]
}

def detect_structured_intent(q: str) -> str:
    ql = q.lower()

    # hostel
    if any(k in ql for k in ["contact","phone","email","supervisor","warden","director","manager"]):
        return "contacts"
    if "block" in ql and any(k in ql for k in ["name","names","list","all","codes","code"]):
        return "blocks"
    if any(k in ql for k in ["hostel","room","mess","laundry","mh","lh"]):
        return "hostel"

    # academics
    for tag, words in ACA_KEYWORDS.items():
        if any(w in ql for w in words): return tag

    if any(k in ql for k in ["fee","fees","tuition"]):
        return "tabular"

    return "text"

def parse_filters(q: str) -> Dict[str, Any]:
    ql = q.lower()
    f = {"ay": None, "gender": None, "category": None, "level": None, "program_like": None, "level_like": None}

    m = re.search(r"\b(20\d{2})\b", ql)
    if m:
        yr = m.group(1); f["ay"] = f"{yr}-{str(int(yr[-2:])+1).zfill(2)}"

    if any(k in ql for k in ["boy","boys","men","mh","mens"]): f["gender"] = "Male"
    if any(k in ql for k in ["girl","girls","ladies","lh","women"]): f["gender"] = "Female"

    if "nri" in ql: f["category"] = "NRI"
    elif "foreign" in ql: f["category"] = "Foreign"
    elif "indian" in ql: f["category"] = "Indian"

    if "senior" in ql: f["level"] = "Senior"
    if "first year" in ql or "fresh" in ql: f["level"] = "First-Year"

    m = re.search(r"\b(b\.?tech|m\.?tech|mca|msc|cse|ece|mechanical|biotech|ai|data)\b", ql)
    if m: f["program_like"] = m.group(1).replace(".", "")
    if "ug" in ql: f["level_like"] = "UG"
    if "pg" in ql: f["level_like"] = "PG"
    return f

def _mk():
    con = sqlite3.connect(DB)
    con.row_factory = sqlite3.Row
    return con

def _safe_fetch(con, sql, args) -> List[sqlite3.Row]:
    try: return con.execute(sql, args).fetchall()
    except sqlite3.OperationalError: return []

# -------- HOSTEL --------

def sql_hostel_overview(f: Dict[str,Any], limit_rows: int = 800) -> Dict[str, Any]:
    con = _mk()
    where = ["1=1"]; args=[]
    if f["gender"]:   where.append("IFNULL(b.gender,'')=IFNULL(?, '')");   args.append(f["gender"])
    if f["level"]:    where.append("IFNULL(b.level,'')=IFNULL(?, '')");    args.append(f["level"])
    if f["ay"]:       where.append("IFNULL(hf.ay,'')=IFNULL(?, '')");      args.append(f["ay"])
    if f["category"]: where.append("IFNULL(hf.category,'')=IFNULL(?, '')");args.append(f["category"])
    where.append("""(
        COALESCE(hf.total_fee,'')<>'' OR COALESCE(hf.room_mess_fee,'')<>'' OR
        COALESCE(hf.admission_fee,'')<>'' OR COALESCE(hf.caution_deposit,'')<>'' OR
        COALESCE(hf.occupancy,'')<>'' OR COALESCE(hf.mess_type,'')<>''
    )""")
    sql = f"""
    SELECT b.display_name AS block, b.gender, b.level, b.block_type,
           hf.ay, hf.category, hf.occupancy, hf.ac, hf.mess_type,
           hf.room_mess_fee, hf.admission_fee, hf.caution_deposit, hf.other_fee, hf.total_fee, hf.currency,
           hf.source_file
    FROM hostel_fees hf
    JOIN blocks b ON b.id = hf.block_id
    WHERE {' AND '.join(where)}
    ORDER BY b.block_type, block, hf.occupancy, hf.ac DESC, hf.mess_type
    """
    rows = _safe_fetch(con, sql, args); con.close()
    columns = ["Block","Gender","Level","Type","AY","Category","Occ","AC","Mess",
               "Room+Mess","Admission","Caution","Other","Total","Curr","Source"]
    tbl = {"title":"Hostel Fee Details (Vellore)", "columns":columns, "rows":[]}
    for r in rows[:limit_rows]:
        tbl["rows"].append([
            r["block"] or "", r["gender"] or "", r["level"] or "", r["block_type"] or "",
            r["ay"] or "", r["category"] or "", r["occupancy"] or "",
            "AC" if (r["ac"]==1) else ("Non-AC" if (r["ac"]==0) else ""),
            r["mess_type"] or "", r["room_mess_fee"] or "", r["admission_fee"] or "",
            r["caution_deposit"] or "", r["other_fee"] or "", r["total_fee"] or "",
            r["currency"] or "", r["source_file"] or ""
        ])
    return {"table": tbl, "bullets": []}

def sql_block_contacts(_f: Dict[str,Any]) -> Dict[str,Any]:
    con = _mk()
    rows = _safe_fetch(con, """
        SELECT '' AS block, name, role, phone, email
        FROM hostel_contacts
        ORDER BY role, name
    """, [])
    con.close()
    cols = ["Block","Name","Role","Phone","Email"]
    tbl = {"title":"Hostel Contacts", "columns":cols, "rows":[
        [r["block"], r["name"] or "", r["role"] or "", r["phone"] or "", r["email"] or ""]
        for r in rows
    ]}
    return {"table": tbl, "bullets": []}

def sql_list_blocks(f: Dict[str,Any]) -> Dict[str,Any]:
    con = _mk()
    if f["gender"] == "Female":
        sql = "SELECT block_code AS display_name, 'Female' AS gender FROM lh_blocks ORDER BY block_code"
    elif f["gender"] == "Male":
        sql = "SELECT COALESCE(block_name, block_code) AS display_name, 'Male' AS gender FROM mh_blocks ORDER BY block_code"
    else:
        sql = """
        SELECT COALESCE(block_name, block_code) AS display_name, 'Male' AS gender FROM mh_blocks
        UNION ALL
        SELECT block_code AS display_name, 'Female' AS gender FROM lh_blocks
        ORDER BY gender, display_name
        """
    rows = _safe_fetch(con, sql, []); con.close()
    cols = ["Block","Gender"]
    tbl = {"title":"Hostel Blocks", "columns":cols, "rows":[
        [r["display_name"] or "", r["gender"] or ""]
        for r in rows
    ]}
    return {"table": tbl, "bullets": []}

# -------- ACADEMICS --------

def _like_clause(col: str, val: str, wh: list, args: list):
    if val:
        wh.append(f"LOWER({col}) LIKE ?")
        args.append(f"%{val.lower()}%")

def sql_programs(f: Dict[str,Any], q: str) -> Dict[str,Any]:
    con = _mk()
    where, args = ["1=1"], []
    _like_clause("level", f["level_like"], where, args)
    _like_clause("program", f["program_like"], where, args)
    _like_clause("program", q, where, args)
    rows = _safe_fetch(con, f"""
        SELECT level, program, school, duration, campus, source_file
        FROM programs
        WHERE {' AND '.join(where)}
        ORDER BY level, program
    """, args); con.close()
    cols = ["Level","Program","School","Duration","Campus","Source"]
    return {"table":{"title":"Programs","columns":cols,"rows":[
        [r["level"] or "", r["program"] or "", r["school"] or "", r["duration"] or "", r["campus"] or "", r["source_file"] or ""]
        for r in rows
    ]}, "bullets":[]}

def sql_eligibility(f: Dict[str,Any], q: str) -> Dict[str,Any]:
    con = _mk()
    where, args = ["1=1"], []
    _like_clause("level", f["level_like"], where, args)
    _like_clause("program", f["program_like"], where, args)
    _like_clause("criteria", q, where, args)
    rows = _safe_fetch(con, f"""
        SELECT level, program, criteria, source_file
        FROM eligibility
        WHERE {' AND '.join(where)}
        ORDER BY level, program
    """, args); con.close()
    cols = ["Level","Program","Criteria","Source"]
    return {"table":{"title":"Eligibility","columns":cols,"rows":[
        [r["level"] or "", r["program"] or "", r["criteria"] or "", r["source_file"] or ""]
        for r in rows
    ]}, "bullets":[]}

def sql_documents(f: Dict[str,Any], q: str) -> Dict[str,Any]:
    con = _mk()
    where, args = ["1=1"], []
    _like_clause("level", f["level_like"], where, args)
    _like_clause("program", f["program_like"], where, args)
    _like_clause("item", q, where, args)
    rows = _safe_fetch(con, f"""
        SELECT level, program, item, details, source_file
        FROM documents_required
        WHERE {' AND '.join(where)}
        ORDER BY level, program, item
    """, args); con.close()
    cols = ["Level","Program","Document","Details","Source"]
    return {"table":{"title":"Documents Required","columns":cols,"rows":[
        [r["level"] or "", r["program"] or "", r["item"] or "", r["details"] or "", r["source_file"] or ""]
        for r in rows
    ]}, "bullets":[]}

def sql_academic_fees(f: Dict[str,Any], q: str) -> Dict[str,Any]:
    con = _mk()
    where, args = ["1=1"], []
    _like_clause("program", f["program_like"], where, args)
    if f["level_like"]: where.append("LOWER(level)=?"); args.append(f["level_like"].lower())
    if f["category"]:  where.append("IFNULL(category,'')=IFNULL(?, '')"); args.append(f["category"])
    if f["ay"]:        where.append("IFNULL(ay,'')=IFNULL(?, '')");        args.append(f["ay"])
    _like_clause("program", q, where, args)
    rows = _safe_fetch(con, f"""
        SELECT level, program, category, ay, tuition, one_time, caution, total, currency, source_file
        FROM academic_fees
        WHERE {' AND '.join(where)}
        ORDER BY level, program, category, ay
    """, args); con.close()
    cols = ["Level","Program","Category","AY","Tuition","One-time","Caution","Total","Curr","Source"]
    return {"table":{"title":"Academic Fees","columns":cols,"rows":[
        [r["level"] or "", r["program"] or "", r["category"] or "", r["ay"] or "",
         r["tuition"] or "", r["one_time"] or "", r["caution"] or "", r["total"] or "",
         r["currency"] or "", r["source_file"] or ""]
        for r in rows
    ]}, "bullets":[]}

def sql_scholarships(f: Dict[str,Any], q: str) -> Dict[str,Any]:
    con = _mk()
    where, args = ["1=1"], []
    _like_clause("level", f["level_like"], where, args)
    _like_clause("name", q, where, args)
    rows = _safe_fetch(con, f"""
        SELECT level, name, criteria, amount, currency, source_file
        FROM scholarships
        WHERE {' AND '.join(where)}
        ORDER BY level, name
    """, args); con.close()
    cols = ["Level","Name","Criteria","Amount","Curr","Source"]
    return {"table":{"title":"Scholarships","columns":cols,"rows":[
        [r["level"] or "", r["name"] or "", r["criteria"] or "", r["amount"] or "", r["currency"] or "", r["source_file"] or ""]
        for r in rows
    ]}, "bullets":[]}
ABOVE IS SQL_ROUTER.PY app folder
